{
  "filename": "A_Decomposition_Approach_to_Multi_Vehicle_Cooperat.pdf",
  "text_length": 75539,
  "chunk_count": 13,
  "chunks": [
    "## A Decomposition Approach to Multi-Vehicle Cooperative Control\n\nMatthew Earl [∗] and Raffaello D’Andrea\n\n\nAbstract\n\n\nWe present methods that generate cooperative strategies for multivehicle control problems using a decomposition approach By introducing\na set of tasks to be completed by the team of vehicles and a task execution\nmethod for each vehicle, we decomposed the problem into a combinatorial\ncomponent and a continuous component The continuous component of\nthe problem is captured by task execution, and the combinatorial component is captured by task assignment In this paper, we present a solver\nfor task assignment that generates near-optimal assignments quickly and\ncan be used in real-time applications To motivate our methods, we apply\nthem to an adversarial game between two teams of vehicles One team is\ngoverned by simple rules and the other by our algorithms In our study of\nthis game we found phase transitions, showing that the task assignment\nproblem is most difficult to solve when the capabilities of the adversaries\nare comparable Finally, we implement our algorithms in a multi-level architecture with a variable replanning rate at each level to provide feedback\non a dynamically changing and uncertain environment ### 1 Introduction\n\n\nUsing a team of vehicles to accomplish an objective can be effective for problems\ninvolving a set of tasks distributed in space and time Examples of such problems include multi-target intercept [1], terrain mapping [24], reconnaissance [28],\nand surveillance [36] To achieve effective solutions, in general, a vehicle team\nneeds to follow a cooperative policy The generation of such a policy has been\nthe subject of a rich literature in cooperative control A sample of the noteworthy work in this field includes a language for modeling and programming\ncooperative control systems [18], receding horizon control for multi-vehicle systems [9], non-communicative multi-robot coordination [21], hierarchical methods for target assignment and intercept [1], cooperative estimation for reconnaissance problems [28], mixed integer linear programming methods for cooperative\ncontrol [33, 13], the compilation on multi-robots in dynamics environments [22],\nand the compilation on cooperative control and optimization [26] ∗ Corresponding author email: mge1@cornell edu\n\n\n1\n\n\nWhen multi-vehicle teams operate in dynamically changing and uncertain\nenvironments, which is often the case, a model predictive approach [23] can\nbe used to provide feedback This approach involves frequently recomputing\nthe team control policy in real-time However, because these systems are often\nhybrid dynamical systems, computing a cooperative policy is often computationally hard The challenge addressed in this paper is to (1) develop a method\nto generate near-optimal cooperative policies quickly and (2) to effectively implement the method In our previous work on cooperative control [13, 11] we developed mixed\ninteger linear programming methods because of their expressiveness and ease\nof modeling many types of problems The drawback is that real-time planning\nis infeasible because of the computational complexity of the approach This\nmotivated us to develop a trajectory primitive decomposition approach to the\nproblem This approach finds near-optimal solutions quickly, allowing real-time\nimplementation, and can be tuned to balance the tradeoff between optimality\nand computational effort for the particular problem at hand The drawback,\ncompared to our previous work, is that it is limited to cooperative control problems in which vehicle tasks can be clearly defined and efficient primitives exist In this paper, we present our trajectory primitive decomposition approach We analyze the average case behavior of the approach by solving instances of a\ncooperative control problem derived from Cornell’s RoboFlag environment And\nfinally, we implement the approach in a hierarchical architecture with variable\nreplanning rates at each level and test the implementation in a dynamically\nchanging and uncertain RoboFlag environment The trajectory primitive decomposition involves the introduction of a set of\ntasks to be executed by the vehicles, allowing the problem to be separated into a\nlow-level component, called task execution, and a high-level component, called\ntask assignment The task execution component is formulated as an optimal\ncontrol problem, which explicitly involves the vehicle dynamics Given a vehicle\nand a task, the goal is to find the control inputs necessary to execute the given\ntask in an optimal way The task assignment component is an NP-hard [14]\ncombinatorial optimization problem The goal is to assign a sequence of tasks to\neach vehicle so that the team objective is optimized Task assignment does not\nexplicitly involve the vehicle dynamics because the task execution component\nis utilized as a trajectory primitive We have developed a branch and bound algorithm to solve the task assignment problem One of the benefits of this algorithm is that it can be stopped\nat any time in the solution process and the output is the best feasible assignment found in that time This is advantageous for real-time applications where\ncontrol strategies must be generated within a time window In this case, the\nbest solution found in the time window is used Another advantage is that the\nalgorithm is complete; given enough time, it will find the optimal solution To analyze the average case performance of the branch and bound solver, we\ngenerate and solve many instances of the problem We look at computational\ncomplexity, convergence to the optimal assignment, and performance variations\nwith parameter changes We found that the solver converges to the optimal\n\n\n2\n\n\nassignment quickly However, the solver takes much more time to prove the\nassignment is optimal Therefore, if the solver is terminated early, the best\nfeasible assignment found in that time is likely to be a good one We also found\nseveral phase transitions in the task assignment problem, similar to those found\nin the pioneering work [20, 34, 25] At the phase transition point, the task\nassignment problem is much harder to solve For cooperative control problems\ninvolving adversaries, the transition point occurs when the capabilities of the two\nteams are comparable This behavior is similar to the complexity of balanced\ngames like chess [16] Finally, we implement the methods in a multi-level architecture with replanning occurring at each level, at different rates (multi-level model predictive\ncontrol) The motivation is to provide feedback to help handle dynamically\nchanging environments",
    "The motivation is to provide feedback to help handle dynamically\nchanging environments The paper is organized as follows: In Section 2, we state the multi-vehicle\ncooperative control problem and introduce the decomposition In Section 3, we\nintroduce the example problem used to motivate our approach In Section 4,\nwe describe our solver for the task assignment problem, and in Section 5, we\nanalyze its average case behavior Finally, in Section 6, we apply our solver in\na dynamically changing and uncertain environment using a multi-level model\npredictive control architecture for feedback A web page that accompanies this\npaper can be found at [10] ### 2 Multi-vehicle task assignment\n\n\nThe general multi-vehicle cooperative control problem consists of a heterogeneous set of vehicles (the team), an operating environment, operating constraints, and an objective function The goal is to generate a team strategy\nthat minimizes the objective function The strategy in its lowest level form is\nthe control inputs to each vehicle of the team In [11, 13], we show how to solve this problem using hybrid systems tools This approach is successful in determining optimal strategies for complex multivehicle problems, but becomes computationally intensive for large problems Motivated to find faster techniques, we have developed a decomposition approach described in this paper The key to the decomposition is to introduce a relevant set of tasks for the\nproblem being considered Using these tasks, the problem can be decomposed\ninto a task completion component and a task assignment component The task\ncompletion component is a low level problem, which involves a vehicle and a\ntask to be completed The task assignment component is a high level problem,\nwhich involves the assignment of a sequence of tasks to be completed by each\nvehicle in the team Task Completion: Given a vehicle, an operating environment with constraints, a task to be completed, and an objective function, find the control\ninputs to the vehicle such that the constraints are satisfied, the task is completed, and the objective is minimized 3\n\n\nFigure 1: The framework for the task assignment problem using task completion\nprimitives Task Assignment : Given a set of vehicles, a task completion algorithm for\neach vehicle, a set of tasks to be completed, and an objective function, assign a\nsequence of tasks to each vehicle such that the objective function is minimized In the task assignment problem, instead of varying the control inputs to\nthe vehicles to find an optimal strategy, we vary the sequence of tasks assigned\nto each vehicle This problem is a combinatorial optimization problem and\ndoes not explicitly involve the dynamics of the vehicles However, in order to\ncalculate the objective function for any particular assignment, we must use the\ntask completion algorithm Task completion acts as a primitive in solving the\ntask assignment problem, as shown by the framework in Figure 1 Using the low\nlevel component (task completion), the high level component (task assignment)\nneed not explicitly consider the detailed dynamics of the vehicles required to\nperform a task ### 3 RoboFlag Drill\n\n\nTo motivate and make concrete our decomposition approach, we illustrate the\napproach on an example problem derived from Cornell’s multi-vehicle system\ncalled RoboFlag For an introduction to RoboFlag, see the papers from the\ninvited session on RoboFlag in the Proceedings of the 2003 American Control\nConference [3, 7, 8] In [19], protocols for the RoboFlag Drill are analyzed using\na computation and control language The RoboFlag Drill involves two teams of vehicles, the defenders and the\nattackers, on a playing field with a circular region of radius R dz at its center\ncalled the Defense Zone (Figure 2) The attackers’ objective is to fill the Defense\nZone with as many attackers as possible They have a fixed strategy in which\neach moves toward the Defense Zone at constant velocity An attacker stops if\nit is intercepted by a defender or if it enters the Defense Zone The defenders’\nobjective is to deny as many attackers as possible from entering the Defense\nZone without entering the zone themselves A defender denies an attacker from\nthe Defense Zone by intercepting the attacker before it reaches the Defense Zone The wheeled vehicles of Cornell’s RoboCup Team [37, 6] are the defenders in\nthe RoboFlag Drill problem we consider in this paper Each vehicle is equipped\n\n\n4\n\n\nFigure 2: The RoboFlag Drill used to motivate the methods presented is this\npaper The drill takes place on a playing field with a Defense Zone at its\ncenter The objective is to design a cooperative control strategy for the team\nof defending vehicles (black) that minimizes the number of attacking vehicles\n(white) that enter the Defense Zone with a three-motor omni-directional drive that allows it to move along any\ndirection irrespective of its orientation This allows for superior maneuverability\ncompared to traditional nonholonomic (car-like) vehicles A local control system\non the vehicle, presented in [27] and Appendix A, alters the dynamics so that\nat a higher level of the hierarchy, the vehicle dynamics are governed by\n\n\nx¨(t) + ˙x(t) = u x (t)\n\ny¨(t) + ˙y(t) = u y (t)\nu x (t) [2] + u y (t) [2] ≤ 1 (1)\n\n\nThe state vector is x = (x, y, ˙x, ˙y), and the control input vector is u = (u x, u y ) These equations are less complex than the nonlinear governing equations of\nthe vehicles They allow for the generation of feasible near-optimal trajectories\nwith little computational effort and have been used successfully in the RoboCup\ncompetition Each attacker has two discrete modes: active and inactive When active, the\nattacker moves toward the Defense Zone at constant velocity along a straight\nline path The attacker, which is initially active, transitions to inactive mode\nif the defender intercepts it or if it enters the Defense Zone Once inactive, the\nattacker does not move and remains inactive for the remainder of play These\ndynamics are captured by the discrete time equations\n\n\np[k + 1] = p[k] + v p T a[k]\n\n\n5\n\n\n(in Defense Zone)\n\n\notherwise any\n\n\nFigure 3: The two state (active and inactive) attacker state machine The\nattacker starts in the active state It transitions to the inactive state, and\nremains in this state, if it enters the Defense Zone or if it is intercepted by a\ndefender q[k + 1] = q[k] + v q T a[k] (2)\n\n\nand the state machine (see Figure 3)\n\n\n\na[k + 1] =\n\n\n\n\n\n\n\n\n1 if (a[k] = 1)\nand (not in Defense Zone)\nand (not intercepted)\n0 if (a[k] = 0)\nor (in Defense Zone)\nor (intercepted)\n\n\n\n(3)\n\n\n\nfor all k in the set {1, , N a } The initial conditions are\n\n\np[0] = p s, q[0] = q s, and a[0] = 1 (4)\n\n\nIn these equations, N a is the number of samples, T is the sample time, (p[k], q[k])\nis the attacker’s position at time t a [k] = kT, (v p, v q ) is its constant velocity\nvector, and a[k] ∈{0, 1} is a discrete state indicating the attacker’s mode The\nattacker is active when a[k] = 1 and inactive when a[k] = 0 Given (p[k], q[k])\nand a[k], we can calculate the attacker’s position at any time t, denoted p(t) =\n(p(t), q(t)), using the equations\n\n\np(t) = p[k] + v p a[k](t − t a [k])\nq(t) = q[k] + v q a[k](t − t a [k]), (5)\n\n\nwhere k = ⌊t/T ⌋ Because the goal of the RoboFlag Drill is to keep attackers out of the Defense\nZone, attacker intercept is an obvious task for this problem Therefore, the task\ncompletion problem for the RoboFlag Drill is an intercept problem RoboFlag Drill Attacker Intercept (RDAI): Given a defender with state x(t)\ngoverned by equation (1) with initial condition x(0) = x s, an attacker governed\nby equations (2) and (3) with initial conditions given by (4) and coordinates p(t)\ngiven by equation (5), obstacles and restricted regions to be avoided, time dependent final condition x(t f ) = (p(t f ), q(t f ), 0, 0), and objective function J T C = t f,\nfind the control inputs to the defender that minimize the objective such that\nthe constraints are satisfied 6\n\n\nThe operating environment includes the playing field and the group of attacking vehicles The operating constraints include collision avoidance between\nvehicles and avoidance of the Defense Zone (for the defending robots) Next, we define notation for a primitive that generates a trajectory solving\nthe RDAI problem The inputs to the primitive are the current state of defender\nd, denoted x d (t), and the current position of attacker a, denoted p a (t) The\noutput is the amount of time it takes defender d to intercept attacker a, denoted\n∆t int (d, a, t), given by\n\n\n∆t int (d, a, t) := intTime[x d (t), p a (t)] (6)\n\n\nIf defender d can not intercept attacker a before the attacker enters the Defense\nZone, we set ∆t int (d, a, t) := ∞ Near-optimal solutions to the RDAI problem can be generated using the\ntechnique presented in [27] with straightforward modification The advantage\nof this technique is that it finds very good solutions quickly, which allows for\nthe exploration of many trajectories in the planning process Another way to\ngenerate near-optimal solutions for RDAI is to use the iterative mixed integer linear programming techniques presented in [12] The advantage of this\napproach is that it can handle complex hybrid dynamical systems Either of\nthese approaches could be used as a primitive for the RDAI problem Using the\nprimitive, the RoboFlag Drill problem can be expressed as the following task\nassignment problem:\nRoboFlag Drill Task Assignment (RDTA): Given a team of defending vehicles D = {d 1, , d n }, a set of attackers to intercept A = {a 1, , a m }, initial\nconditions for each defender d and for each attacker a, an RDAI primitive, and\nan objective function J, assign each defender d in D a sequence of attackers to\nintercept, denoted α d, such that the objective function is minimized We introduce notation (listed in Table 1) to describe the cost function J\nand the algorithm that solves the RDTA problem Let m d be the number\nof attackers assigned to defender d, and let α d = ⟨α d (1), , α d (m d )⟩ be the\nsequence of attackers defender d is assigned to intercept Let t d (i) be the time\nat which defender d completes the ith task in its task sequence α d Let A u be\nthe set of unassigned attackers, then A −A u is the set of assigned attackers An assignment for the RDTA problem is an intercept sequence α d for each\ndefender d in D A partial assignment is an assignment such that A u is not\nempty, and a complete assignment is an assignment such that A u is empty The set of times {t d (i) : i = 1, , m d }, for each defender d, are computed\nusing the primitive in equation (6) The time at which defender d intercepts\nthe ith attacker in its intercept sequence, if not empty, is given by\n\n\nt d (i − 1), if ∆t int (d, α d (i), t d (i − 1)) = ∞\nt d (i) =\n� t d (i − 1) + ∆t int (d, α d (i), t d (i − 1)), otherwise [,]\n\n\nwhere we take t d (0) = 0 If defender d can not intercept attacker α d (i) before\nthe attacker enters the Defense Zone, the time t d (i) is not incremented because,\n\n\n7\n\n\nTable 1: Variables for RoboFlag Drill problems\nn number of defending vehicles\nm number of attacking vehicles\nD the set of defending vehicles\nA the set of attacking vehicles\nA u the set of unassigned attacking vehicles\nA −A u the set of assigned attacking vehicles\nx d (t) the state of defender d at time t\np a (t) the position of attacker a at time t\nα d the sequence of attackers assigned to defender d\nm d the length of defender d’s intercept sequence α d\n∆t int (d, a, t) time needed for d to intercept a starting at time t t d (i) the time that d completes ith task in task sequence α d\nγ a binary variable indicating if a enters the Defense Zone\nJ the cost function for the RDTA problem\nǫ weight in the cost function J\n\n\nin this case, the defender does not attempt to intercept the attacker The time\nat which defender d completes its intercept sequence α d is given by t d (m d ) To indicate if attacker a enters the Defense Zone during the drill, we introduce binary variable γ a given by\n\n\n1 if attacker a enters Defense Zone\nγ a = (7)\n0 otherwise �\n\n\nIf γ a = 1, attacker a enters the Defense Zone at some time during play, otherwise,\nγ a = 0 and attacker a is intercepted We compute γ a for each attacker a in the\nset of assigned attackers (A −A u ) as follows: For each d in D and for each i in\n{1, , m d }, if ∆t int (d, α d (i), t d (i − 1)) = ∞ then set γ α d (i) = 1, otherwise set\nγ α d (i) = 0 For the RDTA problem, the cost function has two components The primary\ncomponent is the number of assigned attackers that enter the Defense Zone\nduring the drill,\n\n\nJ 1 = � γ a (8)\n\na∈(A−A u )\n\n\nThe secondary component is the time at which all assigned attackers that do\nnot enter the Defense Zone (all a such that γ a = 0) are intercepted,\n\n\nJ 2 = max d∈D [t] [d] [(][m] [d] [)][ ] (9)\n\n\nThe weighted combination is\n\n\nJ = � γ a + ǫ max d∈D [t] [d] [(][m] [d] [)][,] (10)\n\na∈(A−A u )\n\n\n8\n\n\nwhere we take 0 < ǫ ≪ 1 because we want the primary component to dominate In particular, keeping attackers out of the Defense Zone is most important Therefore, our goal in the RDTA problem is to generate a complete assignment\n(A u empty) that minimizes equation (10) ### 4 Branch and bound solver\n\n\nOne way to find the optimal assignment for RDTA is by exhaustive search; try\nevery possible assignment of tasks to vehicles and pick the one that minimizes J This approach quickly becomes computationally infeasible for large problems As the number of tasks or the number of vehicles increase, the total number\nof possible assignments grows significantly A more efficient solution method\nis needed for real-time planning With this motivation, we developed a branch\nand bound solver for the problem In this section, we describe the solver and\nits four major components: node expansion, branching, upper bound, and lower\nbound We use a search tree to enumerate all possible assignments for the problem The root node represents the empty assignment, all interior nodes represent\npartial assignments, and the leaves represent the set of all possible complete assignments Given a node representing a partial assignment, the node expansion\nalgorithm (Section 4 1) generates the node’s children Using the node expansion\nalgorithm, we grow the search tree starting from the root node The branching algorithm (Section 4 2) is used to determine the order in which nodes are\nexpanded In this algorithm, we use A* search [35] to guide the growth of the\ntree toward good solutions Given a node in the tree representing a partial assignment, the upper bound\nalgorithm (Section 4 3) assigns the unassigned attackers in a greedy way The\nresult is a feasible assignment The cost of this assignment is an upper bound on\nthe optimal cost that can be achieved from the given node’s partial assignment The upper bound is computed at each node explored in the tree (not all nodes\nare explored, many are pruned) As the tree is explored, the best upper bound\nfound to date is stored in memory Given a node in the search tree representing a partial assignment, the lower\nbound algorithm (Section 4 4) assigns the unassigned attackers in A using the\nprinciple of simultaneity Each defender is allowed to pursue multiple attackers\nsimultaneously Because this is physically impossible, the resulting assignment\nis potentially infeasible Because no feasible assignment can do better, the cost\nof this assignment is a lower bound on the cost that can be achieved from the\ngiven node’s partial assignment Similar to the upper bound, the lower bound\nis computed at each node explored in the tree If the lower bound for the current node being explored is greater or equal\nto the best upper bound found, we prune the node from the tree, eliminating\nall nodes that emanate from the current node This can be done because of\n\nthe way we have constructed the tree The task sequences that make up a\nparent’s assignment are subsequences of the sequences that make up each child’s\n\n\n9\n\n\nTable 2: Branch and bound algorithm\n\n\n1: Start with a tree containing only the root node 2: Run upper bound algorithm with root node’s partial\nassignment (the empty assignment) as input, generating a feasible complete assignment 3: Set J ub [best] to the cost the complete assignment 4: Expand the root node using expand node routine 5: while growing the tree do\n\n6: Use branching routine to pick next branch to explore 7: Use upper bound algorithm to compute feasible\ncomplete assignment from current node’s partial\nassignment, and set the cost of this assignment to\nJ ub 8: if J ub < J ub [best] [, set][ J] ub [best] := J ub 9: Use lower bound algorithm to calculate the lower\nbound cost from the current node’s partial assignment, and set this cost to J lb 10: if J lb ≥ J ub [best] [, prune current node from the tree ]\n\n11: end while\n\n\nassignment Therefore, exploring the descendants will not result in a better\nassignment than that already obtained Before we describe the details of the components, we describe the branch and\nbound algorithm listed in Table 2 Start with the root node, which represents\nthe empty assignment, and apply the upper bound algorithm This generates\na feasible assignment with cost denoted J ub [best] because it is the best, and only,\nfeasible solution generated so far Next, apply the node expansion algorithm to\nroot, generating its children At this point, enter a loop For each iteration of the loop, apply the branching algorithm to select the node to explore next The node selected by the\nbranching algorithm, which we call the current node, contains a partial assignment Apply the upper bound algorithm to the current node, generating a\nfeasible complete assignment with cost denoted J ub If J ub is less than J ub [best] [,]\nwe have found a better feasible assignment so we set J ub [best] := J ub Next, apply\nthe lower bound algorithm to generate an optimistic cost, denoted J lb, from the\ncurrent node’s partial assignment If J lb is greater than or equal to the best\nfeasible cost found so far J ub [best] [, prune the node from the tree, removing all of]\nits descendants from the search We do not need to consider the descendants of\n\nthis node because doing so will not result in a better feasible assignment than\nthe one found already, with cost J ub [best] [ The loop continues until all nodes have]\nbeen explored or pruned away The result is the optimal assignment for the\n\n\n10\n\n\n8\n\n\n6\n\n\n4\n\n\n2\n\n\n0\n\n\n−2\n\n\n−4\n\n\n−6\n\n\n\n\n\n\n\n8\n\n\n6\n\n\n4\n\n\n2\n\n\n0\n\n\n−2\n\n\n−4\n\n\n−6\n\n\n\n\n\n\n\n−8\n−5 0 5\n\n\n\n−8\n−5 0 5\n\n\n\nFigure 4: The solution to two instances of the RDTA problem using the branch\nand bound solver The circle at the center of the field is the Defense Zone The lines with asterisks denote the attacker trajectories, and the lines without\ndenote defender trajectories The parameters for these instances are ǫ = 0 01,\nn = 3, and m = 6 RDTA problem In Figure 4, we plot the solution to two instances of the RDTA problem\nsolved using the branch and bound solver Notice that the defenders work\ntogether and do not greedily pursue the attackers that are closest For example,\nin the figure on the left, defenders 2 and 3 ignore the closest attackers and\npursue attackers further away for the benefit of the team In the remainder of this section we describe the components of the branch\nand bound solver in detail 4 1 Node expansion\n\n\nHere we describe the node expansion algorithm used to grow a search tree\nthat enumerates all possible assignments for the RDTA problem Each node\nof the tree represents an assignment Starting from the root node, attackers\nare assigned, forming new nodes, until all complete assignments are generated Each node represents a partial assignment except for the leaves, which represent\nthe set of complete assignments Consider the case with one defender D = {d 1 } and three attackers A =\n{a 1, a 2, a 3 } The tree for this case is shown in Figure 5 To generate this tree,\nwe start from the root node representing the empty assignment, denoted ⟨⟩ We\nexpand the root node generating three children, each representing an assignment\ncontaining a single attacker to intercept The children are then expanded, and\nso on, until all possible assignments are generated For multiple defenders, unlike the single defender case, the tree is unbalanced\nto avoid redundancies For example, consider the case with two defenders D =\n{d 1, d 2 } and two attackers A = {a 1, a 2 } The tree for this case is shown in\nFigure 6 Again, each node represents an assignment, but now the assignment\nis a sequence of attackers to intercept for each defender in D In general, for\n\n\n11\n\n\n⟨a 3 ⟩\n\n\n\n⟨a 1 ⟩\n\n\n\nα d 1 = ⟨⟩\n\n\n⟨a 2 ⟩\n\n\n\n⟨a 1, a 2 ⟩\n\n\n⟨a 1, a 2, a 3 ⟩\n\n\n\n⟨a 1, a 3 ⟩\n\n\n⟨a 1, a 3, a 2 ⟩\n\n\n\n⟨a 2, a 1 ⟩\n\n\n⟨a 2, a 1, a 3 ⟩\n\n\n\n⟨a 2, a 3 ⟩\n\n\n⟨a 2, a 3, a 1 ⟩\n\n\n\n⟨a 3, a 1 ⟩\n\n\n⟨a 3, a 1, a 2 ⟩\n\n\n\n⟨a 3, a 2 ⟩\n\n\n⟨a 3, a 2, a 1 ⟩\n\n\n\nFigure 5: Search tree for the RDTA problem with the defender set D = {d 1 }\nand the attacker set A = {a 1, a 2, a 3 } Each node of the tree denotes a sequence\nof attackers to be intercepted by defender d 1 The root node is the empty\nassignment The leaves of the tree give all possible complete assignments of\nattackers in A α d 1 = ⟨⟩\nα d 2 = ⟨⟩\n\n\n\n⟨a 1 ⟩\n⟨⟩\n\n\n⟨a 1, a 2 ⟩ ⟨a 1 ⟩\n⟨⟩ ⟨a 2 ⟩\n\n\n\n⟨a 2 ⟩\n⟨⟩\n\n\n⟨a 2, a 1 ⟩ ⟨a 2 ⟩\n⟨⟩ ⟨a 1 ⟩\n\n\n\n⟨⟩\n⟨a 1 ⟩\n\n\n⟨⟩\n⟨a 1, a 2 ⟩\n\n\n\n⟨⟩\n⟨a 2 ⟩\n\n\n⟨⟩\n⟨a 2, a 1 ⟩\n\n\n\nFigure 6: Search tree for the RDTA problem with defender set D = {d 1, d 2 }\nand the attacker set A = {a 1, a 2 } Each node of the tree denotes a sequence\nof attackers to intercept for defender d 1 and defender d 2 The root node is the\nempty assignment The leaves of the tree give all possible complete assignments\nof attackers in A 12\n\n\ndefender set D with n defenders and attacker set A with m attackers there are\n(n + m − 1) /(n − 1) complete assignments (or leaves in the search tree) To generate a search tree for the general case, we use a node expansion algorithm This algorithm takes any node and generates the node’s children The\nassignment for each child is constructed by appending an unassigned attacker\nto one of the sequences in the parent node’s assignment The task sequences in\nthe parent’s assignment are always subsequences of the sequences in its child’s\nassignment Therefore, when we prune a node from the search tree, we can also\nprune all of its descendants The node expansion algorithm uses a different representation for an assignment than we have used thus far We introduce this new representation\nwith an example involving the defender set D = {d 1, d 2 } and the attacker set\nA = {a 1, a 2, , a 7 } Consider a partial assignment given by\n\n\nα d 1 = ⟨a 4, a 1 ⟩\n\n\nα d 2 = ⟨a 2, a 5, a 7 ⟩ In this case, attackers a 3 and a 6 have yet to be assigned Our node expansion\nalgorithm represents this partial assignment with the vectors\n\n\nδ = (1, 1, 2, 2, 2, 0, 0)\n\n\nβ = (4, 1, 2, 5, 7, 0, 0), (11)\n\n\nboth of length m = 7 Vector δ holds defender indices and vector β holds\nattacker indices For a unique representation, the elements in δ are ordered so\nthat δ(i) ≤ δ(i + 1) For the example case, attackers a β(1) and a β(2) (i e , a 4\nand a 1 ) are assigned to defender d 1 in sequence, and attackers a β(3), a β(4), a β(5)\n(i e , a 2, a 5, a 7 ) are assigned to defender d 2 in sequence In general, the input to the node expansion algorithm is a parent node with\nassignment give by\n\n\nparent δ = (δ(1), δ(2), , δ(p), 0, , 0)\nparent β = (β(1), β(2), , β(p), 0, , 0), (12)\n\n\nwhere both vectors are of size m, and p is the number of tasks already assigned\n(or the number of nonzero entries in each vector) The output is a set of N child\nchildren, where\n\n\nN child = (n − δ(p) + 1)(m − p) (13)\n\n\nEach child has assignment vectors δ and β identical to its parent except for\nentries δ(p + 1) and β(p + 1) In the child’s assignment, attacker a β(p+1) is\nappended to defender d δ(p+1) ’s sequence of attackers to intercept α δ(p+1) The\ndetails of the node expansion algorithm are given in Table 3 To demonstrate the node expansion algorithm, we expand the node given\nby equation (11) as shown in Figure 7 Figure 8 shows the normal notation for\nthis expansion",
    "Figure 8 shows the normal notation for\nthis expansion Using this algorithm, we can grow the assignment tree for any\nRDTA problem In Figure 9 we show the tree for the two vehicle two attacker\nexample written in our node expansion algorithm’s notation 13\n\n\nTable 3: Node expansion algorithm\n\n\n1: k := 1\n\n2: for i = δ(p), δ(p + 1), , n do\n\n3: for each j in the set {{1, , m} −\n{β(1), β(2), , β(p)}} do\n\n4: child(k) δ = parent δ\n\n5: child(k) β = parent β\n6: child(k) δ(p + 1) = i\n\n7: child(k) β(p + 1) = j\n8: k := k + 1\n\n\n9: end for\n\n\n10: end for\n\n\nδ = (1, 1, 2, 2, 2, 0, 0)\nβ = (4, 1, 2, 5, 7, 0, 0)\n\n\nδ = (1, 1, 2, 2, 2, 2, 0) δ = (1, 1, 2, 2, 2, 2, 0)\nβ = (4, 1, 2, 5, 7, 3, 0) β = (4, 1, 2, 5, 7, 6, 0)\n\n\nFigure 7: The node from equation (11), written in node expansion format, is\nexpanded using the node expansion algorithm in Table 3 α d 1 = ⟨a 4, a 1 ⟩\nα d 2 = ⟨a 2, a 5, a 7 ⟩\n\n\nα d 1 = ⟨a 4, a 1 ⟩ α d 1 = ⟨a 4, a 1 ⟩\nα d 2 = ⟨a 2, a 5, a 7, a 3 ⟩ α d 2 = ⟨a 2, a 5, a 7, a 3, a 6 ⟩\n\n\nFigure 8: The expansion in Figure 7 in our original notation 14\n\n\n(2, 0)\n(2, 0)\n\n\n(2, 2)\n(2, 1)\n\n\n\n(1, 0)\n(1, 0)\n\n\n(1, 1) (1, 2)\n(1, 2) (1, 2)\n\n\n\nδ = (0, 0)\nβ = (0, 0)\n\n\n(1, 0)\n(2, 0)\n\n\n(1, 1) (1, 2)\n(2, 1) (2, 1)\n\n\n\n(2, 0)\n(1, 0)\n\n\n(2, 2)\n(1, 2)\n\n\n\nFigure 9: The tree from Figure 6 written using our node expansion algorithm’s\nnotation n 0\n\n\n\nn 1\n\n\nn 4 n 5\n\n\n\nn 2\n\n\nn 6 n 7\n\n\n\nn 3\n\n\nn 8 n 9\n\n\n\nFigure 10: Example search tree used to illustrate the branching routine in the\nbranch and bound solver 4 2 Search algorithm\n\n\nTo determine the order in which we expand nodes, we have tried several tree\nsearch algorithms including the systematic search algorithms breadth first search\n(BFS), depth first search (DFS) [5], and A* search [35] The A* search algorithm\norders nodes according to a heuristic branching function to help guide the search\ntoward the optimal assignment We use the upper bound algorithm presented\nin Section 4 3 as the branching function The lower bound algorithm presented\nin Section 4 4 could also be used as the branching function For example, consider a tree with three levels, where node i is labeled n i as\nshown in Figure 10 For this tree, BFS gives the ordering (n 0, n 1, n 2, n 3, n 4, n 5, n 6, n 7, n 8, n 9 ),\nand DFS gives the ordering (n 0, n 1, n 4, n 5, n 2, n 6, n 7, n 3, n 8, n 9 ) Suppose the\nupper bound algorithm run at each node i gives the following results: J ub (n 1 ) =\n3, J ub (n 2 ) = 1, J ub (n 3 ) = 2, J ub (n 4 ) = 2, J ub (n 5 ) = 1, J ub (n 6 ) = 1, J ub (n 7 ) = 1,\nJ ub (n 8 ) = 1, J ub (n 9 ) = 0 A* BFS gives the ordering (n 0, n 2, n 3, n 1, n 6, n 7, n 9, n 8, n 5, n 4 ),\nand A* DFS gives the ordering (n 0, n 2, n 6, n 7, n 3, n 9, n 8, n 1, n 5, n 4 ) In A* search, the children of a node must be sorted with respect to the\nbranching function The maximum number of children that emanate from any\ngiven node is the nm children emanating from the root node Therefore, the\nmaximum number of items that need to be sorted is nm To sort the children,\nwe use Shell’s method [29], which runs in O((nm) [3][/][2] ) time 15\n\n\n4 3 Upper bound algorithm\n\n\nIn this section, we describe a fast algorithm that generates a feasible complete\nassignment given any partial assignment The cost of the resulting complete\nassignment is an upper bound on the optimal cost that can be achieved from\nthe given partial assignment The idea behind the upper bound algorithm is\nto assign unassigned attackers in a greedy way At each step, we assign the\nattacker defender pair that results in the minimum intercept time We proceed\nuntil all attackers are assigned or until none of the remaining attackers can be\nintercepted before entering the Defense Zone The details of this algorithm,\nwhich runs in O(nm [2] ) time, are listed in Table 4 The input to the algorithm is a partial assignment given by an intercept\nsequence α d for each defender d in D such that the set of unassigned attackers\nA u is not empty In addition, we take as inputs the variables associated with this\npartial assignment including the time for defender d to complete its intercept\nsequence α d, given by t d (m d ), and binary variable γ a for each a in the set of\nassigned attackers A −A u Given a partial assignment, the greedy step of the algorithm determines the\nattacker in the set A u that can be intercepted in the minimum amount of time,\ndenoted a [∗] The corresponding defender that intercepts a [∗] is denoted d [∗] To\ndetermine this defender, attacker pair (d [∗], a [∗] ) we form a matrix C of intercept\ntimes The matrix has size |D| × |A u |, and its elements are given by\n\n\nc(d, a) := t d (m d ) + ∆t int (d, a, t d (m d )), (14)\n\n\nfor each d in D and a in A u The element c(d, a) is the time it would take\ndefender d to complete its intercept sequence α d and then intercept attacker a The minimum of these times gives the desired defender, attacker pair\n\n\nc(d [∗], a [∗] ) = min (15)\nd∈D,a∈A u [c][(][d, a][)][ ]\n\n\nIf c(d [∗], a [∗] ) = ∞, no attacker can be intercepted before it enters the Defense\nZone Thus, we set γ a := 1 for each a in A u Then, we set A u to the empty\nset because all attackers are effectively assigned, and we use equation (10) to\ncalculate the upper bound J ub Otherwise, c(d [∗], a [∗] ) is finite, and we add attacker\na [∗] to defender d [∗] ’s intercept sequence by incrementing m d ∗ by one and setting\nα d [∗] (m d [∗] ) := a [∗] Then, because a [∗] has now been assigned, we remove it from\nthe set of unassigned attackers by setting A u := A u −{a [∗] } If A u is not empty,\nwe have a new partial assignment, and we repeat the procedure Otherwise, the\nassignment is complete and we use equation (10) to compute the upper bound\nJ ub 4 4 Lower bound algorithm\n\n\nHere we describe a fast algorithm that generates a lower bound on the cost\nthat can be achieved from any given partial assignment The idea behind the\nalgorithm is to use the principle of simultaneity In assigning attackers from\n\n\n16\n\n\nTable 4: Greedy upper bound algorithm\n\n\n1: Given a partial assignment: intercept sequence α d for\neach d ∈D, a nonempty set of unassigned attackers\nA u, t d (m d ) for each d ∈D, and γ a for each a ∈\n(A −A u ) 2: Initialize variables for unassigned attackers Set\nγ a := 0 for each a in A u 3: Calculate the elements of matrix C For all d ∈D\nand a ∈A u, set\n\n\nc(d, a) := t d (m d ) + ∆t int (d, a, t d (m d )) 4: while A u not empty do\n\n5: Find minimum element of C given by\n\n\nc(d [∗], a [∗] ) = min\nd∈D,a∈A u [c][(][d, a][)][ ]\n\n\n6: If c(d [∗], a [∗] ) = ∞, no attacker in the set A u can\nbe intercepted before entering the Defense Zone Break out of the while loop 7: Append attacker a [∗] to defender d [∗] ’s assignment by\nsetting m d ∗ := m d ∗ + 1 and α d ∗ (m d ∗ ) := a [∗] 8: Update finishing time for d [∗] by setting t d ∗ (m d ∗ ) :=\nc(d [∗], a [∗] ) 9: Remove a [∗] from consideration since it has been\nassigned Set c(d, a [∗] ) to ∞ for all d ∈D, and set\nA u := A u −{a [∗] } 10: Update matrix for defender d [∗] For all attackers\na ∈A u, set\n\n\nc(d [∗], a) := t d [∗] (m d [∗] ) + ∆t int (d [∗], a, t d [∗] (m d [∗] )) 11: end while\n\n\n12: For each a in A u, set γ a := 1 13: Set\n\n\nJ ub := � γ a + ǫ max d∈D [t] [d] [(][m] [d] [)][ ]\n\na∈A\n\n\n17\n\n\nA u, we assume each defender can pursue multiple attackers simultaneously The result is a potentially infeasible complete assignment because simultaneity\nis physically impossible Because no feasible assignment can do better, the cost\nof this assignment is a lower bound on the optimal cost that can be achieved\nfrom the given partial assignment The algorithm, which runs in O(nm) time,\nis listed in Table 5 Similar to the upper bound algorithm, the input to the lower bound algorithm is a partial assignment This includes an intercept sequence α d for each\ndefender d in D with A u nonempty, t d (m d ) for each defender d in D, and γ a for\neach attacker a in A −A u Each attacker a in A u is assigned a defender as follows: Form a matrix C\nwith elements\n\n\nc(d, a) := t d (m d ) + ∆t int (d, a, t d (m d )), (16)\n\n\nfor all d in D and a in A u Element c(d, a) is equal to the time it takes d to\nintercept the attackers in its intercept sequence α d plus the time it would take to\nsubsequently intercept attacker a For each a in A u, find the defender, denoted\nd [∗], that can intercept a in minimal time\n\n\nc(d [∗], a) = min (17)\nd∈D [c][(][d, a][)][ ]\n\n\nIf c(d [∗], a) = ∞, we set γ a := 1 because no defender can intercept attacker a\nbefore it enters the Defense Zone Otherwise, we set γ a := 0 because defender\nd [∗] can intercept attacker a before it enters the Defense Zone The lower bound\nis therefore give by\n\n\n\nJ lb := � γ a + ǫ {a∈A max :γ a =0}\n\na∈A\n\n\n\nmin (18)\n� d∈D [c][(][d, a][)] �\n\n\n### 5 Analysis of the solver\n\nIn this section, we explore the average case computational complexity of the\nbranch and bound algorithm by solving randomly generated instances Each\ninstance is generated by randomly selecting parameters from a uniform distribution over the intervals defined below The computations were performed on a\nPC with Intel PIII 550MHz processor, 1024KB cache, 3 8GB RAM, and Linux For all instances solved, processor speed was the limiting factor, not memory",
    "For all instances solved, processor speed was the limiting factor, not memory 5 1 Generating random instances\n\n\nThe initial position of each attacker is taken to be in an annulus centered on\nthe playing field The radius of the initial position, denoted r a, is chosen at\nrandom from a uniform distribution over the interval [r a [min], r a [max] ] The angle of\nthe initial position, denoted θ a, is chosen from a uniform distribution over the\n\n\n18\n\n\nTable 5: Lower bound algorithm\n\n\n1: Given a partial assignment: intercept sequence α d for\neach d ∈D, a nonempty set of unassigned attackers\nA u, t d (m d ) for each d ∈D, and γ a for each a ∈\n(A −A u ) 2: Calculate the elements of matrix C For all d ∈D\nand a ∈A u, set\n\n\nc(d, a) := t d (m d ) + ∆t int (d, a, t d (m d )) 3: for all a ∈A u do\n4: Find minimum element of ath column of C given\nby\n\n\nc(d [∗], a) = min\nd∈D [c][(][d, a][)][ ]\n\n\n5: if c(d [∗], a) = ∞ then set γ a := 1 6: else set γ a := 0 7: end for\n\n\n8: Set\n\n\n\nJ lb := � γ a + ǫ {a∈A max :γ a =0}\n\na∈A\n\n\n19\n\n\n\nmin � d∈D [c][(][d, a][)] �\n\n\ninterval (0, 2π] (all other angles used in this section φ a, θ d, and φ d are also chosen\nfrom a uniform distribution over the interval (0, 2π]) The magnitude of attacker\na’s velocity, denoted v a, is chosen at random from a uniform distribution over\nthe interval [v a [min], v a [max] ] The initial state of the attacker is given by\n\n\np(0) = r a cos(θ a ), q(0) = r a sin(θ a )\n\n˙\np = v a cos(φ a ), ˙q = v a sin(φ a ) (19)\n\n\nThe initial position of each defender is taken to be in a smaller annulus, also\ncentered on the playing field The radius of the initial position, denoted r d,\nis chosen at random from a uniform distribution over the interval [r d [min], r d [max] ] The magnitude of defender d’s velocity, denoted v d, is chosen at random from\na uniform distribution over the interval [v d [min], v d [max] ] The initial state of the\ndefender is given by\n\n\nx(0) = r d cos(θ d ), y(0) = r d sin(θ d )\nx˙ (0) = v d cos(φ d ), ˙y(0) = v d sin(φ d ) (20)\n\n\n\nFor the instances generated in this paper, we set R dz = 2 0 and take\nthe parameters from the following intervals: r a ∈ [7 5, 15 0], v d = 1 0, r d ∈\n\n[√2R dz, 2√2R dz ], and v d ∈ [0 5, 1 0] In Section 5 3, we study the RDTA prob\nlem with variations in the velocity parameters v a and v d [max]",
    "3, we study the RDTA prob\nlem with variations in the velocity parameters v a and v d [max] 5 2 Average case computational complexity\n\n\nIn this section, we present the results of an average case computational complexity study on the branch and bound solver A particular problem instance is\nconsidered solved when the strategy that minimizes the cost is found In Figure 11, we plot the fraction of instances solved versus computation time In the\nfigure on top, the cost function is the number of attackers that enter the Defense\nZone (ǫ = 0 in equation (10)) Solving these instances becomes computationally\nintensive for modest size problems For example, when n = 3 and m = 5, 80%\nof the instances are solved in 60 seconds or less In the figure on bottom, in addition to the primary component of the cost function, the cost function includes\na secondary component (ǫ = 0 01 in equation (10)) 01 in equation (10)) The secondary component\nis the time it takes to intercept all attackers that can be intercepted Solving\nthese instances of the problem is more computationally intensive than the ǫ = 0\ncase For example, when n = 3 and m = 5, only 40% of the problems are solved\nin 60 seconds or less The increase in average case computational complexity for the ǫ > 0 case\nis expected because the cost function has an additional component to be minimized, which is independent of the primary component In a case where the\nprimary component is at a minimum, the algorithm will proceed until it proves\nthat the combination of primary and secondary components is minimized If it is given enough time, the branch and bound solver finds the optimal\nassignment, but the average case computational complexity is high Therefore,\n\n\n20\n\n\n1\n\n\n0 9\n\n\n0 8\n\n\n0 7\n\n\n0 6\n\n\n0 5\n\n\n0 4\n\n\n0 3\n\n\n0 2\n\n\n0 1\n\n\n\n\n\n\n\n0\n0 10 20 30 40 50 60 70 80 90\n\ntime (sec)\n\n\n\n1\n\n\n0 9\n\n\n0 8\n\n\n0 7\n\n\n0 6\n\n\n0 5\n\n\n0 4\n\n\n0 3\n\n\n0 2\n\n\n0 1\n\n\n\n\n\n\n\n0\n0 10 20 30 40 50 60 70 80 90\n\ntime (sec)\n\n\nFigure 11: The fraction of instances solved versus computation time for the\nbranch and bound solver On top, the cost is the number of attackers that enter\nthe Defense Zone (ǫ = 0 in equation (10)), and on bottom, the cost includes a\nsecondary component (ǫ = 0 01 in equation (10)) For each curve, 400 random\ninstances of the RDTA problem were solved The values of the parameters are\nn = 3 and m = 3, 4, 5 21\n\n\n35\n\n\n30\n\n\n\n\n\n25\n\n\n20\n\n\n15\n\n\n10\n\n\n5\n\n\n0\n0 50 100 150 200 250 300\n\nbranches (k)\n\n\nFigure 12: The average convergence rate for the branch and bound solver using\neach of the three branching routines BFS, DFS, and A* search We plot the\npercent difference from optimal PD(k) versus the number of branches k explored For each curve 400 random instances of RDTA were solved The parameters\nvalues are ǫ = 0 01, n = 3, and m = 5 using the algorithm to solve for the optimal assignment in real-time is infeasible\nfor most applications However, the best assignment found in the allotted time\nwindow for planning could be used in place of the optimal assignment In\nthis case, it is desirable that the algorithm converge to a near-optimal solution\nquickly To learn more about the convergence rate of the branch and bound solver,\nwe look at the rate at which the best upper bound J ub [best] decreases with branches\ntaken in the search tree Because the branch and bound algorithm is an exact\nmethod, J ub [best] eventually converges to J opt We define the percent difference\nfrom optimal as follows: Let J opt [(][i][)] [be the optimal cost for instance][ i][ Let][ J] ub [(][i][)] [(][k][)]\nbe the best upper bound found after k branches for instance i Let J [ˆ] opt be\nthe mean of the set {J opt [(][i][)] [:][ i][ = 1][, , N] [}][, and let ˆ][J] [ub] [(][k][) be the mean of the]\n\nset {J ub [(][i][)] [(][k][) :][ i][ = 1][, , N] [}][, where][ N][ is the number of instances The percent]\ndifference from optimal is given by\n\n\n\nˆ ˆ\nJ ub (k) − J opt\nPD(k) = 100 ˆ (21)\n\nJ opt\n\n\n\nIn Figure 12, we plot PD(k) versus the number of branches (k) for instances\ninvolving three defenders (n = 3) and five attackers (m = 5) At the root node\n(k = 1), the greedy algorithm is applied Exploration of the tree does not occur\nat this point Therefore, the three branching routines produce the same result,\nPD(1) = 33% This means that J [ˆ] ub (1) − J [ˆ] opt = 0 33 J [ˆ] opt, or J [ˆ] ub (1) = 1 33 J [ˆ] opt 22\n\n\nIn other words, the average cost of the assignment generated by the greedy\nalgorithm is 1 33 times the average optimal cost At one branch into the tree\n(k = 2), both DFS and BFS generate assignments with PD(2) = 28%, and the\nA* search generates assignments with PD(2) = 5% Therefore, after only two\nsteps, the branch and bound algorithm using A* search generates an assignment\nthat, on average, has cost only 1 05 times the cost of the optimal assignment For the instances solved here, the branch and bound solver with A* search\nconverges to the optimal assignment in an average of 8 branches, and it takes\nan average of 740 branches to prove that the assignment is optimal Therefore,\nthe solver converges to the optimal solution quickly, and the computational\ncomplexity that we observed (Figure 11) is due to the time needed to prove\noptimality These results are encouraging for real-time implementation of the algorithm The results show that a very good assignment is generated after a short number\nof branches There is a trade-off between optimality and computation time that\ncan be tuned by deciding how deep into the tree to explore Going deeper into\nthe tree will generate assignments that are closer to optimal, but at the same\ntime, results in an increased computational burden The parameter to be tuned\nis the maximum number of branches to allow the search procedure to explore,\ndenoted kMax To study the computational complexity as kMax is tuned, we look at versions\nof the algorithm (using A*) with kMax = 1 (greedy algorithm), kMax = 2, and\nkMax = ∞ (exact algorithm) These three cases generate assignments with\naverage percent difference from optimal given by PD(1)=33%, PD(2)=5%, and\nPD(∞)=0% respectively The results are shown in Figure 13 The algorithm\nwith kMax = 2 gives a good balance between optimality and computation time",
    "The algorithm\nwith kMax = 2 gives a good balance between optimality and computation time 5 3 Phase Transitions\n\n\nThe RDTA problem is NP-hard [14], which can be shown by reduction using the\ntraveling salesman problem This is a worst case result that says nothing about\nthe average case complexity of the algorithm or the complexity with parameter\nvariations In this section, we study the complexity of the RDTA problem as\nparameters are varied We perform this study on the decision version of the\nproblem RoboFlag Drill Decision Problem (RDD): Given a set of defenders D and a\nset of attackers A, is there a complete assignment such that no attacker enters\nthe Defense Zone First, we consider variations in the ratio of attacker velocity to maximum\ndefender velocity, denoted vA/vD in this section When the ratio is small, the\ndefenders are much faster than the attackers It should be easy to quickly find an\nassignment such that all attackers are intercepted When the ratio is large, the\nattackers are much faster than the defenders In this case, it is difficult for the\ndefenders to intercept all of the attackers, which should be easy to determine The interesting question is whether there is a transition from being able to\nintercept all the attackers (all yes answers to the RDD problem) to not being\n\n\n23\n\n\n1\n\n\n0 9\n\n\n0 8\n\n\n0 7\n\n\n\n\n\n\n\n\n\n0 6\n\n\n0 5\n\n\n0 4\n\n\n0 3\n\n\n0 2\n\n\n0 1\n\n\n100 −1 10 0 10 1 10 2 10 3\n\ntime (sec)\n\n\nFigure 13: The fraction of instances solved versus computation time for the\nbranch and bound solver (using A*) with kMax = 1, 2, and ∞ The kMax variable controls the maximum number of branches explored We vary it from\nkMax = 1, which is a greedy search, to kMax = ∞, which is exhaustive search For each curve, 400 random instances of the RDTA problem was solved For\nthese problems the parameter values are ǫ = 0 01, n = 3, and m = 5 able to intercept all attackers (all no answers to the RDD problem)",
    "able to intercept all attackers (all no answers to the RDD problem) Is this\ntransition sharp Are there values of the ratio for which solving the RDD is\ndifficult For each value of the velocity ratio, we generated random instances of the\nRDD problem and solved them with the branch and bound solver The results\nare shown in Figure 14 The figure on top shows the fraction of instances that\nevaluate to yes versus the velocity ratio The figure on bottom shows the mean\nnumber of branches required to solve an instance versus the velocity ratio There\nis a sharp transition from all instances yes to all instances no This transition\noccurs approximately at vA/vD = 1 for the n = 3, m = 5 case At this value\nof the ratio, there is a spike in computational complexity This easy-hard-easy\nbehavior is indicative of a phase transition [2, 25] We also study the RDD problem with variations in the ratio of defenders\nto attackers, denoted n/m, with vD = vA = 1 For small values of n/m, the\nnumber of attackers is much larger than the number of defenders, and it should\nbe easy to determine that the team of defenders cannot intercept all of the\nattackers In this case, most instances should evaluate to no For large values\nof n/m, the number of defenders is much larger than the number of attackers,\nand it should be easy to find an assignment in which all attackers are denied\nfrom the Defense Zone In this case, most instances should evaluate to yes The\nresults are shown in Figure 15, where it is clear that our expectations proved\ncorrect In between the extremes of the n/m ratio, there is a phase transition\n\n\n24\n\n\n1\n\n\n0 9\n\n\n0 8\n\n\n0 7\n\n\n0 6\n\n\n0 5\n\n\n0 4\n\n\n0 3\n\n\n0 2\n\n\n0 1\n\n\n\n\n\n0\n0 0 5 1 1 5 2 2 5 3\n\n\n_vA/vD_\n\n\n\n550\n\n\n500\n\n\n450\n\n\n400\n\n\n350\n\n\n300\n\n\n250\n\n\n200\n\n\n150\n\n\n100\n\n\n\n\n\n50\n\n\n0\n0 0 5 1 1 5 2 2 5 3\n\n_vA/vD_\n\n\nFigure 14: The phase transition of the RDD problem in the ratio of attacker\nvelocity to maximum defender velocity (vA/vD) The figure on top shows the\nfraction of instances that evaluate to yes versus the velocity ratio The figure\non bottom shows the mean number of branches needed to solve the problem\nversus the velocity ratio The phase transition occurs at a velocity ration of\napproximately 1 For each curve, 100 random instances of the RDD problem\nwere solved In these figures, n = 3 25\n\n\n1\n\n\n\n\n\n0 5\n\n\n0\n**0** **0 2** **0 4** **0 6** **0 8** **1** **1 2** **1 4** **1 6**\n\n\n\n800\n\n\n700\n\n\n600\n\n\n500\n\n\n400\n\n\n300\n\n\n200\n\n\n100\n\n\n0\n\n\n\nn/m\n\n\nFigure 15: The phase transition of the RDD problem in the ratio of defenders\nto attackers (n/m) The solid line shows the fraction of instances that evaluate\nto yes versus the ratio The dashed line shows the mean number of branches\nneeded to solve the problem versus the ratio For each curve, 100 random\ninstances of the RDD problem were solved The velocities are vD = vA = 1 at a ratio of approximately n/m = 0",
    "at a ratio of approximately n/m = 0 65 In general, these experiments show that when one side dominates the other\n(in terms of the number of vehicles or in terms of the capabilities of the vehicles)\nthe RDD problem is easy to solve When the capabilities are comparable (similar\nnumbers of vehicles, similar performance of the vehicles), the RDD is much\nharder to solve This behavior is similar to the complexity of balanced games\nlike chess [16] In Section 7, we discuss how knowledge of the phase transition\ncan be exploited to reduce computational complexity ### 6 Multi-level implementation\n\n\nNow that we have a fast solver that generates near-optimal assignments, we\ntest it in a dynamically changing environment We consider the RoboFlag Drill\nproblem with attackers that have a simple noncooperative strategy built in,\nwhich is unknown to the defenders The hope is that frequent replanning, at all\nlevels of the hierarchical decomposition, will mitigate our assumption that the\nattackers move with constant velocity We use a multi-level receding horizon architecture, shown in Figure 16, to\ngenerate the defenders’ strategy The task assignment module at the top level\nimplements the branch and bound algorithm presented in this paper It generates the assignment α d for each defender d, sending new assignments to the\nmiddle level of the hierarchy at the rate R T A Therefore, the algorithm returns\nthe best assignment computed in the time window 1/R T A There is a task completion module for each defender at the middle level of the\n\n\n26\n\n\nR T A\n\n\nR T C\n\n\nFigure 16: The multi-level architecture for the defending vehicles used in our\nimplementation hierarchy, which receives an updated assignment α d from the task assignment\nmodule at the rate R T A At a rate R T C, the task completion module generates a\ntrajectory from defender d’s current state to a point that will intercept attacker\nα d (1) assuming the attacker moves at constant velocity If attacker α d (1) is\nintercepted, a trajectory to intercept attacker α d (2) is generated, and so on The vehicle module at the bottom of the hierarchy receives an updated\ntrajectory from the task completion module at the rate R T C The module\npropels the vehicle along this trajectory until it receives an update The attackers are taken to be the same vehicles as the defenders (described\nin Appendix A) For the attacker intelligence, we use the architecture shown\nin Figure 17 The levels of the hierarchy are decoupled, so each attacker acts\nindependently The simple intelligence for each attacker is contained in the top\nlevel of the hierarchy The primary objective is to arrive at the origin of the field\nin minimum time However, the attacker tries to avoid the defenders if they get\ntoo close The radius of each defender is artificially enlarged by a factor β > 1 If\nthe artificially enlarged defenders obstruct an attacker’s path toward the origin,\nthe attacker treats them as obstacles, finding a destination that results in an\nobstacle free path The destination is found using a simple reactive obstacle\navoidance routine used in RoboCup [6, 37] The attacker intelligence module\nruns at the rate R I The trajectory generation module at the middle of the hierarchy receives an\nupdated destination at the rate R I The module generates a trajectory from\nthe current state of the attacker to the destination with zero final velocity at\nthe rate R T G, using techniques from [27] The vehicle module at the bottom\nlevel of the hierarchy is the same as that for the defenders Because the algorithms are more computationally intensive for the higher\nlevels of the hierarchy than the lower levels, the rates are constrained as follows: R T A < R T C and R I < R T G In the simulations that follow, we take\nR T C = R T G because the middle levels of the two hierarchies are comparative\ncomputationally We also set R I = R T G /10 Therefore, if the trajectory generation module replans every time unit, the attacker intelligence module replans\n\n\n27\n\n\nR I\n\n\nR T G\n\n\nFigure 17: The multi-vehicle architecture for the attackers used to test the\ndefender architecture every ten time units First, note that when both R T A and R T C are zero, there is no replanning In this case, all attackers usually enter the Defense Zone They easily avoid the\ndefenders because the defenders execute a fixed plan, which becomes obsolete\nonce the attackers start using their intelligence Next, we present simulation results of the RoboFlag Drill with intelligent\nattackers and defenders We consider problems with eight defenders (n = 8),\nfour attackers (n = 4), vA = vD, R T C = R T G - 0, and R I = R T G /10 We\nconsider several different values of the rate at which the task assignment module\nreplans (R T A ) For each value, we solve 200 randomly generated instances of\nthe problem As an evaluation metric, we use the average number of attackers\nthat enter the Defense Zone during play",
    "As an evaluation metric, we use the average number of attackers\nthat enter the Defense Zone during play For the case R T A = 0, there is no replanning at the task assignment level Replanning only occurs at the task completion level The defenders are given a\nplan from the task assignment module at the beginning of play Each defender\nexecutes its assignment throughout, periodically recalculating the trajectory it\nmust follow to intercept the next attacker in its sequence For this case, on\naverage, 58% of the attackers enter the Defense Zone during play For the case R T A  - 0, replanning occurs at both the task assignment level\nand the task completion level of the hierarchy In addition to recomputing\ntrajectories to intercept the next attacker in each defender’s assignment, the\ndefender assignments are recomputed This redistributes tasks based on the\ncurrent state of the dynamically changing environment, providing feedback This redistributes tasks based on the\ncurrent state of the dynamically changing environment, providing feedback For\nR T A = R T C /40, R T C /20, and R T C /15, an average of 38%, 34%, and 32 5%\nof the attackers enter the Defense Zone during play, respectively Therefore,\nreplanning at the task assignment level has helped increase the utility of the\nstrategies generated for the team of defenders In Figure 18, we show snapshots of an instance of the RoboFlag Drill simulation for the case where the defenders do not replan at the task assignment\nlevel In this case, all attackers enter the Defense Zone In Figure 19, we show\nsnapshots of the same instance of the RoboFlag Drill simulation, but in this\ncase, the defenders replan at the task assignment level (R T A = R T C /15) The\n\n\n28\n\n\nFigure 18: Snapshots of the RoboFlag Drill simulation with defender replanning\nat the task completion level only (R T A = 0) In this case, all attackers enter\nthe Defense Zone The large circles are the defenders, and the small circles are\nthe attackers The solid lines are trajectories Each cross connected to a dashed\nline is an attacker’s desired destination defenders cooperate to deny all attackers from the Defense Zone For example,\nthe two defenders at the lower left of the field cooperate to intercept an attacker ### 7 Discussion\n\n\nWe developed a decomposition approach that generates cooperative strategies\nfor multi-vehicle control problems, and we motivated the approach using an\nadversarial game called RoboFlag In the game, we fixed the strategy for one\nteam and used our approach to generate strategies for the other team By\nintroducing a set of tasks to be completed by the team and a task completion\nmethod for each vehicle, we decomposed the problem into a high level task\nassignment problem and a low level task completion problem We presented\na branch and bound solver for task assignment, which uses upper and lower\nbounds on the optimal assignment to prune the search space The upper bound\nalgorithm is a greedy algorithm that generates feasible assignments The best\ngreedy assignment is stored in memory during the search, so the algorithm can\nbe stopped at any point in the search and a feasible assignment is available 29\n\n\nFigure 19: Snapshots of the RoboFlag Drill simulation with defender replanning at the task completion level and task assignment level (R T A = R T C /15) Because there is replanning at both levels, the defenders cooperate to intercept\nall attackers The large circles are the defenders, and the small circles are the\nattackers The solid lines are trajectories Each cross connected to a dashed\nline is an attacker’s desired destination 30\n\n\nIn our computational complexity study, we found that solving the task assignment problem is computationally intensive, which was expected because the\nproblem is NP-hard However, we showed that the solver converges to the optimal assignment quickly, and takes much more time to prove the assignment is\noptimal Therefore, the solver can be run in a time window to generate nearoptimal assignments for real-time multi-vehicle strategy generation To increase\nthe speed of the algorithm, it may be advantageous to distribute the computation over the set of vehicles [30], taking advantage of the distributed structure\nof the problem We also studied the computational complexity of the solver as parameters\nwere varied We varied the ratio of the maximum velocities of the opposing\nvehicles, and we varied the ratio of the number of vehicles per team We found\nthat when one team has a capability advantage over the other, such as a higher\nmaximum velocity or more vehicles, the solution to the task assignment problem is easy to generate However, when the teams are comparable in capability,\nfinding the optimal assignment to the problem is much more computationally\nintensive This type of analysis can help in deciding how many vehicles to deploy in an adversarial game and what capabilities the vehicles should have In\naddition, knowledge of the phase transition may be exploited to reduce computational complexity In [31, 32], phase transition ‘backbones’ are exploited\nto decompose combinatorial problems into many separate subproblems, which\nare much less computationally intensive This decomposition is amenable to\nparallel computation In [15], it is shown that the hardness of a problem depends on the parameters of the problem (as we showed above) and the details\nof the algorithm used to solve the problem Therefore, it is possible that the\nhard instances of our problem, which lie along the phase transition, may be\nsolved faster if we use a different solution algorithm The authors in [15] suggest adding randomization to the algorithm and using a rapid restart policy The restart policy selects a new random seed for the algorithm and restarts it\nif the algorithm is not making sufficient progress with the current seed Finally, we demonstrated the effectiveness of our approach in an environment\nwhere the adversaries had a noncooperative intelligence that was unknown We\nfound that the simple model used for the adversaries in the solver could be\nmitigated by a multi-level replanning architecture In this architecture, there\nare two levels: low level task completion and high level task assignment When\nreplanning does not occur at either level, the solver fails because it generates\na plan that becomes obsolete as the adversaries use their intelligence When\nreplanning occurs at the task completion level, an assignment is generated once\nby the solver As the adversaries use their intelligence, the task completion\ncomponent is run periodically for each vehicle, generating a new trajectory to\ncomplete the tasks in the vehicle’s assignment This was somewhat effective\nat handling the unknown intelligence When replanning occurs at both levels, the task assignment component is run periodically in addition to the task\ncompletion component We found this replanning architecture effective at retasking in the dynamically changing environment It is advantageous to replan\nfrequently, on average, but there are instances where replanning frequently is\n\n\n31\n\n\nnot advantageous In these cases, the vehicles are retasked so frequently that\ntheir productivity is reduced Therefore, it may be desirable to place a penalty\non changing each vehicle’s current task In general, we feel the multi-level replanning approach is a natural way to\nhandle multi-vehicle cooperative control problems There are many different\ndirections for further research, including the addition of a high level learning\nmodule to generate better models of the adversaries through experience [38] ### A Vehicle Dynamics\n\n\n\nThe wheeled robots of Cornell’s RoboCup Team [37] are the defenders in the\nRoboFlag problems we consider in this paper We state their governing equations and simplify them by restricting the allowable control inputs [27] The result is a linear set of governing equations coupled by a nonlinear constraint on the\ncontrol input This procedure allows real-time calculation of many near-optimal\ntrajectories and has been successfully used by Cornell’s RoboCup team [37, 27] Each vehicle has a three-motor omni-directional drive which allows it to\n\nmove along any direction irrespective of its orientation This allows for superior\nmaneuverability compared to traditional nonholonomic (car-like) vehicles The\nnondimensional governing equations for each vehicle are given by\n\n\nx¨(t) x˙ (t)\n\n¨ ˙\n\n   \n\n\n\n\n\n\n\nx¨(t)\n\n¨\n\n yθ¨((tt))\n\n\n\n\n\n\n\n+\n\n\n\n\nx˙ (t)\n\n˙\n\n 2mL y( [2] t)˙\n\n J θ\n\n\n\n\n\n\n\nJ θ˙(t)\n\n\n\n= u(θ(t), t), (22)\n\n\n\n\nwhere (x(t), y(t)) are the coordinates of the robot on the playing field, θ(t) is\nthe orientation of the robot, and u(θ(t), t) = P(θ(t))U(t) can be thought of as\na θ(t)-dependent control input, where\n\n\n\nsin(θ) − sin( [π] 3 [−] [θ][)] sin( [π] 3 [+][ θ][)]\n\ncos(θ) − cos( [π] [−] [θ][)] − cos( [π] [+][ θ]\n\n\n\nθ) − cos( [π] 3 [−] [θ][)] − cos( [π] 3 [+][ θ][)]\n\n1 1 1\n\n\n\n\n[π] 3 [−] [θ][)] − cos( [π] 3\n\n\n\n\n[π] 3 [−] [θ][)] sin( [π] 3\n\n\n\n\n\n, (23)\n\n\n\n\nP(θ) =\n\n\n\n\n\n\n\n− sin(θ) − sin( [π] 3\n\n cos(θ) − cos( [π] 3\n\n1 1\n\n\n\n\n\nand\n\n\n\n (24)\n\n\n\n\n\nU(t) =\n\n\n\n UU 12 ((tt))\n\n U 3 (t)\n\n\n\nIn the equations above, m is the mass of the vehicle, J is the vehicle’s moment\nof inertia, L is the distance from the drive to the center of mass, and U i (t) is\nthe voltage applied to motor i By restricting the admissible control inputs we simplify the governing equations in a way that allows near-optimal performance The set of admissible\nvoltages U is given by the unit cube and the set of admissible control inputs\nis given by P (θ)U The restriction involves replacing the set P (θ)U with the\nmaximal θ-independent set found by taking the intersection of all possible sets\n\n\n32\n\n\nof admissible controls This set is characterized by the inequalities\n\n\n\n3 −|u θ (t)|\nu x (t) [2] + u y (t) [2] ≤ � 2\n\n\n\n2\n(25)\n�\n\n\n\nand\n|u θ (t)| ≤ 3, (26)\n\n\nwhere the θ-independent control is given by (u x (t), u y (t), u z (t)) The equations\nof motion become\n\n\nx¨(t) x˙ (t) u x (t)\n\n¨ ˙\n\n     \n\n\n\n\n\n\n\nx¨(t)\n\n¨\n\n yθ¨((tt))\n\n\n\n\n\n\n\n+\n\n\n\n\n\n\n\n\n, (27)\n\n\n\n\nx˙ (t)\n\n˙\n\n 2mL y( [2] t)˙\n\n J θ\n\n\n\n\n\n\n\nJ θ˙(t)\n\n\n\n\n\n\n\n uu xy ((tt))\n\n u θ (t)\n\n\n\n =\n\n\n\nsubject to constraints (25) and (26), which couple the degrees of freedom To\ndecouple the θ dynamics we set |u θ (t)| ≤ 1 Then constraint (25) becomes\n\n\nu x (t) [2] + u y (t) [2] ≤ 1 (28)\n\n\nNow the equations of motion for the translational dynamics of the vehicle are\ngiven by\n\n\nx¨(t) + ˙x(t) = u x (t)\ny¨(t) + ˙y(t) = u y (t), (29)\n\n\nsubject to constraint (28) In state space form we have\n\n\nx˙ (t) = A c x(t) + B c u(t), (30)\n\n\nwhere x = (x, y, ˙x, ˙y) is the state and u = (u x, u y ) is the control input",
    "In state space form we have\n\n\nx˙ (t) = A c x(t) + B c u(t), (30)\n\n\nwhere x = (x, y, ˙x, ˙y) is the state and u = (u x, u y ) is the control input ### References\n\n\n[1] R W Beard, T W McLain, M A Goodrich, and E P P Anderson, “Coordinated Target Assignment and Intercept for Unmanned Air Vehicles,”\nIEEE Trans Robot Automat , vol 18, pp 991–922, Dec 2002 [2] R Bejar, I Vetsikas, C Gomes, Henry Kautz, and B Selman, “Structure\nand Phase Transition Phenomena in the VTC Problem,” In TASK PI\nMeeting Workshop, 2001 Selman, “Structure\nand Phase Transition Phenomena in the VTC Problem,” In TASK PI\nMeeting Workshop, 2001 [3] M Campbell, R D’Andrea, D Schneider, A Chaudhry, S Waydo, J Sullivan, J Veverka, and A Klochko, “RoboFlag Games using Systems Based,\nHierarchical Control,” Proceedings of the American Control Conference,\nJune 4–6, 2003, pp Klochko, “RoboFlag Games using Systems Based,\nHierarchical Control,” Proceedings of the American Control Conference,\nJune 4–6, 2003, pp 661–666 [4] C G Cassandras and W Li, “A Receding Horizon Approach for Solving Some Cooperative Control Problems,” Proc IEEE Conf Decision and\nControl, Las Vegas, Neveda, Dec 2002, pp 3760–3765 33\n\n\n[5] T H Cormen, C E Leiserson, R L Rivest, and C Stein Introduction to\nAlgorithms Second Edition The MIT Press, Cambridge, Massachusetts,\n2001 The MIT Press, Cambridge, Massachusetts,\n2001 [6] R D’Andrea, T Kalm´ar-Nagy, P Ganguly, and M Babish, “The Cornell\nRobocup Team,” In G Kraetzschmar, P Stone, T Balch Eds , Robot Soccer\nWorldCup IV, Lecture Notes in Artificial Intelligence, Springer, 2001 [7] R D’Andrea and R M Murray, “The RoboFlag Competition,” Proceedings\nof the American Control Conference, June 4–6, 2003, pp 650–655 [8] R D’Andrea and M Babish, “The RoboFlag Testbed,” Proceedings of the\nAmerican Control Conference, June 4–6, 2003, pp 656–660 [9] W B Dunbar and R M",
    "M Murray, “Distributed Receding Horizon Control\nwith Application to Multi-Vehicle Formation Stabilization,” Accepted to\nAutomatica, June, 2004 [10] A web page that accompanies this paper can be found at\nhttp://control mae cornell edu/earl/decomp\n\n\n[11] M G Earl and R D’Andrea, “Multi-vehicle Cooperative Control\nUsing Mixed Integer Linear Programming,” preprint available at\nhttp://control mae cornell edu/earl/milp1\n\n\n[12] M G Earl and R Earl and R D’Andrea, “Iterative MILP Methods for Vehicle Control\nProblems,” Proc IEEE Conf IEEE Conf Decision and Control, Atlantis, Paradise\nIsland, Bahamas, Dec 2004 [13] M G Earl and R D’Andrea, “Modeling and Control of a Multi-agent System using Mixed Integer Linear Programming,” Proc IEEE Conf Decision\nand Control, Las Vegas, Neveda, Dec 2002, pp 107–111 [14] M R Garey and D S Johnson Computers And Intractability: A guide to\nthe Theory of NP-Completeness W H Freeman and Company, 1979 [15] C P Gomes, B Selman, N Crato, and H Kautz, “Heavy-tailed Phenomena in Satisfiability and Constraint Satisfaction Problems,” Journal of\nAutomated Reasoning, 24 (1-2): 67–100 FEB 2000 Kautz, “Heavy-tailed Phenomena in Satisfiability and Constraint Satisfaction Problems,” Journal of\nAutomated Reasoning, 24 (1-2): 67–100 FEB 2000 [16] H J van den Herik, J W H M Uiterwijk, J van Rjiswijck, “Games\nSolved: Now and in the Future,” ARTIFICIAL INTELLIGENCE vol van Rjiswijck, “Games\nSolved: Now and in the Future,” ARTIFICIAL INTELLIGENCE vol 134\n(1-2): pp 277-311, Jan 2002 [17] Y Ho and K Chu, “Team Decision Theory and Information Structures in\nOptimal Control Problems – Part 1,” IEEE Trans Automatic Control, vol AC-17, pp 15–22, 1972 [18] E",
    "[18] E Klavins, “A Language for Modeling and Programming Cooperative Control Systems,” Proceedings of the International Conference on Robotics and\nAutomation, 2004 Klavins, “A Language for Modeling and Programming Cooperative Control Systems,” Proceedings of the International Conference on Robotics and\nAutomation, 2004 34\n\n\n[19] E 34\n\n\n[19] E Klavins, 42nd IEEE Conference on Decision and Control, “A Formal\nModel of a Multi-Robot Control and Communication Task,” Maui, HI,\nDecember 2003 Klavins, 42nd IEEE Conference on Decision and Control, “A Formal\nModel of a Multi-Robot Control and Communication Task,” Maui, HI,\nDecember 2003 [20] S Kirkpatrick and B Selman, “Critical-behavior in the Satisfiability of\nRandom Boolean Expressions,” Science, 264 (5163), 1297–1301, MAY 27,\n1994 [21] J R Kok, M T J Spaan, and N Vlassis, “Non-communicative multirobot coordination in dynamic environments,” Robotics and Autonomous\nSystems, 50 (2–3): 99–114, Feb Vlassis, “Non-communicative multirobot coordination in dynamic environments,” Robotics and Autonomous\nSystems, 50 (2–3): 99–114, Feb 28, 2005 [22] P U Lima, F C A Groen, “Special issue on multi-robots in dynamic\nenvironments,” Robotics and Autonomous Systems, 50 (2–3): 81–83, Feb Groen, “Special issue on multi-robots in dynamic\nenvironments,” Robotics and Autonomous Systems, 50 (2–3): 81–83, Feb 28, 2005 [23] D Q Mayne, J B Rawlings, C V Rao, and P O M Scokaert, “Constrained Model Predictive Control: Stability and Optimality,” Automatica,\nvol Scokaert, “Constrained Model Predictive Control: Stability and Optimality,” Automatica,\nvol 36, pp 789–814, 2000 [24] M Di Marco, A Garulli, A Giannitrapani, A Vicino, “Simultaneous Localization and Map Building for a Team of Cooperating Robots: A Set\nMembership Approach,” IEEE Transactions on Robotics and Automation,\nvol 19 (2), pp 238–249, Apr 2003 [25] R Monasson, R Zecchina, S Kirkpatrick, B Selman, and L Troyansky,\n“Determining Computational Complexity from Characteristic ’Phase Transitions’,” Nature, 400(8), 1999 [26] R Murphey and P M Pardalos, Eds , Cooperative Control and Optimization, Boston: Kluwer Academic, 2002",
    ", Cooperative Control and Optimization, Boston: Kluwer Academic, 2002 [27] T Kalm´ar-Nagy, R D’Andrea, and P Ganguly “Near-Optimal Dynamic Trajectory Generation and Control of an Omnidirectional Vehicle,”\nRobotics and Autonomous Systems, vol “Near-Optimal Dynamic Trajectory Generation and Control of an Omnidirectional Vehicle,”\nRobotics and Autonomous Systems, vol 46, pp 47–64, 2004 [28] J Ousingsawat and M E Campbell, “Establishing Optimal Trajectories for\nMulti-vehicle Reconnaissance,” AIAA Guidance, Navigation and Control\nConference, 2004 Campbell, “Establishing Optimal Trajectories for\nMulti-vehicle Reconnaissance,” AIAA Guidance, Navigation and Control\nConference, 2004 [29] W H Press, B P Flannery, S A Teukolsky, and W T Vetterling, Numerical Recipes in C : The Art of Scientific Computing, Cambridge University\nPress, New York, 1992 [30] T L Ralphs, “Parallel Branch and Cut for Capacitated Vehicle Routing,”\nParallel Computing, vol 29(5), pp 607–629, May 2003 [31] J Schneider, C Froschhammer, I Morgenstern, T Husslein, and J M Singer, ”Searching for Backbones—An Efficient Parallel Algorithm for the\nTraveling Salesman Problem,” Computer Physics Communications, 96 (23): 173–188 AUG 1996 35\n\n\n[32] J Schneider, ”Searching for Backbones—A High-performance Parallel Algorithm for Solving Combinatorial Optimization Problems,” Future Generation Computer Systems, 19 (1): 121–131 JAN 2003 Schneider, ”Searching for Backbones—A High-performance Parallel Algorithm for Solving Combinatorial Optimization Problems,” Future Generation Computer Systems, 19 (1): 121–131 JAN 2003 [33] T Schouwenaars, E Feron, B de Moor, and J P How, “Mixed Integer\nProgramming for Multi-vehicle Path Planning,” European Control Conference, September 2001 [34] B Selman and S Kirkpatrick, “Critical Behavior in the Computational\nCost of Satisfiability Testing,” Artificial Intelligence, 81(1-2):273–295,\n1996 Kirkpatrick, “Critical Behavior in the Computational\nCost of Satisfiability Testing,” Artificial Intelligence, 81(1-2):273–295,\n1996 [35] M Stefik Introduction to Knowledge Systems Morgan Kaufmann Publishers, Inc , San Francisco, California, 1995 [36] S A Stoeter, P E Rybski, K N Stubbs, C P McMillen, M Gini, D F Hougen, and N Papanikolopoulos, “A robot team for surveillance tasks:\nDesign and architecture,” Robotics and Autonomous Systems, 40 (2–3):\n173–183 Aug 31, 2002 [37] P Stone, M Asada, T Balch, R D’Andrea, M Fujita, B Hengst, G Kraetzschmar, P Lima, N Lau, H Lund,D Polani,P Scerri, S Tadokoro,T Weigel, and G Wyeth, “RoboCup-2000: The Fourth Robotic Soccer World\nChampionships,” AI MAGAZINE, vol Wyeth, “RoboCup-2000: The Fourth Robotic Soccer World\nChampionships,” AI MAGAZINE, vol 22(1), pp 11–38, Spring 2001 [38] P Stone and M Veloso, “Multiagent Systems: A Survey from a Machine\nLearning Perspective,” Autonomous Robots, vol 8, pp 345–383, 2000 36"
  ]
}