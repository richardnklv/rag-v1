{
  "filename": "Replay_Debugging_of_Complex_Real_Time_Systems__Exp.pdf",
  "text_length": 36423,
  "chunk_count": 14,
  "chunks": [
    "**[AADEBUG2003](http://www elis UGent be/aadebug2003/)** **211**\n\n# **Replay Debugging of** **Complex Real-Time** **Systems: Experiences** **from Two Industrial Case** **Studies**\n\n\nDaniel Sundmark [∗][,][1], Henrik Thane [∗][,][1],\nJoel Huselius [∗][,][1], Anders Pettersson [∗][,][1],\nRoger Mellander [†], Ingemar Reiyer [†],\nMattias Kallvi\n\n\n∗ _Department of Computer Science and Engineering, Mälardalen University, PO Box 883, SE-721 23_\n_Västerås, Sweden_\n\n  - _ABB Robotics Hydrovägen 10, SE-721 68 Västerås, Sweden_\n\n\n**ABSTRACT**\n\n\n**Deterministic replay is a method for allowing complex multitasking real-time systems to be debugged us-**\n**ing standard interactive debuggers Even though several replay techniques have been proposed for parallel,**\n**multi-tasking and real-time systems, the solutions have so far lingered on a prototype academic level, with**\n**very little results to show from actual state-of-the-practice commercial applications This paper describes**\n**a major deterministic replay debugging case study performed on a full-scale industrial robot control sys-**\n**tem, as well as a minor replay instrumentation case study performed on a military aircraft radar system In**\n**this article, we will show that replay debugging is feasible in complex multi-million lines of code software**\n**projects running on top of off-the-shelf real-time operating systems Furthermore, we will discuss how re-**\n**play debugging can be introduced in existing systems without impracticable analysis efforts In addition,**\n**we will present benchmarking results from both studies, indicating that the instrumentation overhead is**\n**acceptable and affordable **\n\n\nKEYWORDS : replay; reproducability; instrumentation; case study\n\n### **1 Introduction**\n\n\nABB is a world leading manufacturer of industrial robots for industrial automation Out of all deployed industrial robots to date, ABB has delivered about 50 percent Out of those 50 percent, 70\npercent are deployed in the car manufacturing industry SAAB Avionics is a major supplier of electronic warfare technology on the international market The main focus of the company is electronic\nwarfare systems, such as display systems, tactical reconnaissance systems and electromagnetic technology services Avionics products can for example be found in the Swedish fighter aircraft Gripen,\nthe American F-15 and the NH- 90 helicopter In M Ronsse, K De Bosschere (eds), proceedings of the Fifth International Workshop on Automated Debugging (AADEBUG 2003), September 2003, Ghent COmputer Research Repository (http://www acm org/corr/), cs SE/yymmnnn; whole\nproceedings: cs SE/0309027 1 E-mail: {daniel sundmark, henrik thane, joel huselius, anders pettersson}@mdh se\n\n\nFifth Int Workshop on Automated and Algorithmic Debugging\n\n\n**212** **DANIEL SUNDMARK ET AL **\n\n\n**1",
    "**\n\n\n**1 1** **Contribution**\n\n\nIn this paper, we present results from two industrial case studies performed in cooperation with the\nabove companies With these case studies, we show that our recent research results have not merely\nbeen academic artifacts, but contributions to a fully operational method of debugging full-scale industrial real-time systems In addition, we present benchmarking results from both case studies,\nshowing that the overhead incorporated in the system by instrumentation is acceptable In addition, we present benchmarking results from both case studies,\nshowing that the overhead incorporated in the system by instrumentation is acceptable **1 2** **Paper Outline**\n\n\nThe rest of this paper is organized as follows: Section 2 will give a background and a motivation to\nthe case studies as well as short descriptions of the systems studied Section 3 describes the implementations of our method in the investigated systems In Section 4, instrumentation benchmarking\nresults are presented Finally, Section 5 and Section 6 conclude the paper and discuss future work ### **2 Background and Motivation**\n\n\nIt is no secret that testing, debugging and maintenance constitute the largest percentage of the overall\ncost of an average industrial software project In a recent study, NIST [Rep02] has shown that more\nthan $59 billion/year is spent on debugging software in the U S A alone alone As the average complexity\nof software applications increase constantly it is now common that testing and debugging constitute\nmore than 80% of the total life cycle cost [Rep02] A known fact is also that bugs are introduced\nearly in the design but not detected until much later downstream in the development cycle, typically\nduring integration, and early customer acceptance test For embedded real-time software this fact\nmakes the situation really difficult, since most failures that are detected during integration and early\ndeployment are extremely difficult to reproduce This makes debugging of embedded real-time systems costly since repetitive reproductions of the failure is necessary in order to find the bug The lack\nof proper tools and methods for testing and debugging of complex real-time systems does not help\nthe situation The reason why ABB Robotics and SAAB Avionics systems were chosen as case study\nsubjects was based on their high level of software- and overall technical complexity In addition, the\nsystems operate in safety-critical and high availability environments, where failures might be very\ncostly, making system validation and verification even more important **2",
    "**2 1** **Replay Debugging**\n\n\nDuring the mid-eighties, in an effort to address the problems with inadequate tools for debugging\nof complex systems, LeBlanc and Mellor-Crummey proposed a method of recording information\nduring run-time and using this information to reproduce the exact behavior of the execution offline [LMC87] This method, called _Instant Replay_, allowed otherwise unfit cyclic debugging techniques to be used for debugging nondeterministic systems Instant Replay, as many of its successors\n\n[AL94][CAN [+] 01][RDB99][TCO91][CdKF00], was focused on debugging of non-real-time concurrent\nprograms, thereby concentrating mainly on the correct reproduction of synchronization races and,\nin some cases, on-the-fly detection of data races However, some methods for debugging of complex real-time systems have also been proposed [DR92][TFCB90] These methods have called for the\navailability of specialized hardware and non standard instrumentation tools in order to work satisfactorily They have also been mere academic prototypes and not suited for the complexity of real\nworld software applications They have also been mere academic prototypes and not suited for the complexity of real\nworld software applications Fifth Int Workshop on Automated and Algorithmic Debugging\n\n\n**REPLAY DEBUGGING OF COMPLEX REAL-TIME SYSTEMS: EXPERIENCES FROM TWO**\n\n**INDUSTRIAL CASE STUDIES** **213**\n\n\n**2 2** **Real-Time System Debugging using Time Machines and Deterministic Re-**\n**play**\n\n\nIn our previous work on debugging of embedded real-time systems, we have proposed a replay\nmethod for recording important events and data during a _reference execution_ and to reproduce the\nexecution in a subsequent _replay execution_ [TH00][TSHP03][Hus02] As for most replay debugging\nmethods, if the reference execution fails, the replay execution can be used to reproduce the failure\nrepeatedly in a debugger environment in order to track down the bug Our method allows replay\nof real-time applications running on top of standard commercial real-time operating systems (RTOS)\nand uses standard cyclic debuggers for sequential software There is no need for specialized hardware or specialized compilers for the method to work and the software based instrumentation overhead has so far proven acceptable This allows our probes to be left permanently in the system,\nmaking post-mortem debugging of a deployed system possible while at the same time eliminating\nthe risk of experiencing _probe effects_ [Gai86] during debugging We refer to our method as _Determinis-_\n_tic Replay_ The tool that is used to “travel back in time” and investigate what sequence of events that\nled to a failure is referred to as the _Time Machine_ and consists of three major parts: The _Recorder_ which\nis the instrumentation mechanism (similar to the black-box or flight recorder in an airplane), the _His-_\n_torian_ which is the post-mortem off-line analysis tool and the _Time Traveler_ which is the mechanism\nthat forces the system to behave exactly as the reference execution off-line The main objective of performing the case studies was to validate the applicability of the Time\nMachine method to existing complex real-time systems The basic questions were:\n\n\n  - Would it be possible to reproduce the behavior of such complex target systems - How do we minimize the analysis and implementation effort required in order to capture the\ndata required from such complex target applications - Would the instrumentation overhead (execution time and data bandwidth) be sufficiently low\nfor introduction in real-world applications",
    "- Would the instrumentation overhead (execution time and data bandwidth) be sufficiently low\nfor introduction in real-world applications **2 3** **ABB Robotics System Model**\n\n\nTo validate our ideas, we had the opportunity to work with a robotics system that is state-of-the-art\nin industrial manufacturing automation The system is a highly complex control system application\nrunning on top of the commercial Wind River RTOS VxWorks However, in order to avoid the somewhat impossible mission of analyzing and instrumenting an approximate of 2 5 million lines of code\nin an academic project, we focused on instrumenting five central parts in the target system:\n\n\n  - The operating system - The application’s operating system abstraction layer - The inter-process communication abstraction layer - The peripheral I/O abstraction layer - The state preserving structures of each individual task All instrumentation was system wide and application-transparent except for the state preserving\nstructures in the tasks A more thorough description of the instrumentation will be given below For\nan overview, see Figure 1 Out of an approximate total of 70 tasks, we choose to record the state (internal static variables)\nof the three most frequently running tasks Thus, we limited the instrumentation efforts for parts of\nthe data flow recording However, these three tasks constituted the major part (approximately 60%)\nof the CPU utilization and their data flow constituted more than 96% of the overall system data flow\nbandwidth However, these three tasks constituted the major part (approximately 60%)\nof the CPU utilization and their data flow constituted more than 96% of the overall system data flow\nbandwidth Fifth Int Workshop on Automated and Algorithmic Debugging\n\n\n**214** **DANIEL SUNDMARK ET AL **\n\n\n**2 3 1** **Robotics System-Level Control Flow Model**\n\n\nIn general, task activations in the robotics system are dependent on message queues Basically, each\ntask is set to block on a specific message queue until a message is received in that queue When a\nmessage is received, the task is activated, performs some action, and is finally set to wait for another\nmessage to arrive Chains of messages and task activations, in turn, are initiated by occurrence of\nexternal events, such as hardware interrupts at arrival of peripheral input",
    "Chains of messages and task activations, in turn, are initiated by occurrence of\nexternal events, such as hardware interrupts at arrival of peripheral input **2 3 2** **Robotics Data Flow Model**\n\n\nWe divide the Robotics data flow model into three parts: The per-task state preserving structures,\nthe inter-process communication and peripheral I/O As for the state preserving structures, each\ntask has its own local data structure used to keep track of the current task state This structure holds\ninformation of message data, static variables and external feedback Naturally, this structure alters\nduring execution as the state of the task changes The inter-process communication is handled by the use of an IPC layer implemented on top of the\nmessage queue primitives in VxWorks This layer extends the functionality of the original message\nqueue mechanisms Finally, peripheral I/O, such as motion control feedback is received through a\nperipheral I/O layer Finally, peripheral I/O, such as motion control feedback is received through a\nperipheral I/O layer **2 4** **SAAB Avionics System Model**\n\n\nIn addition to the Robotics system, we also performed a minor case study in a military aircraft radar\nsystem In short, the task of the system is to warn the pilot of surrounding radar stations and to offer\ncountermeasures This study was less extensive in that it only covered a part of the instrumentation\naspect of the Time Machine technology The full scale Saab Avionics radar system holds about 90\nADA tasks running on top of the Wind River VxWorks RTOS However, in the scope of the case\nstudy, a reduced system with 20 central tasks was investigated Dataflow was limited to inter process messages As in the robotics case, task activations in the\nAvionics system are controlled by message arrivals on certain queues ### **3 Technique Implementations**\n\n\nTo be able to facilitate replay, there were three things we needed to achieve: First, we needed to instrument both the VxWorks real-time kernel and the application source codes in order to be able to\nextract sufficient information of the reference execution Second, we needed to incorporate the replay\nfunctionality into the WindRiver integrated development environment (IDE) We used the _Tornado 2_\nversion of the IDE as this is the standard IDE for developing VxWorks real-time applications Third,\nwe needed to add the Time Machine mechanisms used to perform the actual replay of the system Even though the Tornado 2 IDE features a VxWorks-level simulator, both recording and replay execution were performed on the actual target system In the next sections, we will discuss these steps\none by one **3 1** **VxWorks Instrumentation**\n\n\nA common denominator for the Robotics system and the Avionics system is that they both run on\ntop of the Wind River VxWorks RTOS In order to extract the exact sequence of task interleavings, it\nwas essential to be able to instrument the mechanisms in VxWorks that directly influence the systemlevel control flow Therefore, we instrumented semaphore _wait_ and _signal_ operations, message queue\nblocking _send_ and _receive_, task sleep function _taskDelay_ as well as preemptive scheduling decisions",
    "Therefore, we instrumented semaphore _wait_ and _signal_ operations, message queue\nblocking _send_ and _receive_, task sleep function _taskDelay_ as well as preemptive scheduling decisions Fifth Int Workshop on Automated and Algorithmic Debugging\n\n\n**REPLAY DEBUGGING OF COMPLEX REAL-TIME SYSTEMS: EXPERIENCES FROM TWO**\n\n**INDUSTRIAL CASE STUDIES** **215**\n\n\n**3 1 1 1** **Blocking System Call Instrumentation Layer**\n\n\nTo instrument the blocking task delay-, semaphore- and message queue primitives mentioned above,\nwe added an instrumentation layer on top of the VxWorks system call API This layer replaces the\nordinary primitives with wrappers including the added functionality of instrumentation Apart from\nthis instrumentation, the wrappers use the original primitives for system call operation Apart from\nthis instrumentation, the wrappers use the original primitives for system call operation **3 1 1 2** **Task Switch Hook**\n\n\nWith the blocking system call instrumentation layer, we are able to monitor and log what possibly\nblocking system calls are invoked However, in order to see which of the invoked calls actually leads\nto task interleavings, we need to be able to insert probes into the scheduling mechanisms of the\nkernel Fortunately, VxWorks provides several hooks, implemented as empty callback functions included in kernel mechanisms, such as a TaskSwitchHook in the task switch routine These hooks can\nbe used for instrumentation purposes, making it possible to monitor and log sufficient information\nof each task switch in order to be able to reproduce it These hooks can\nbe used for instrumentation purposes, making it possible to monitor and log sufficient information\nof each task switch in order to be able to reproduce it **3 1 3** **Preemption Instrumentation**\n\n\nAs some of the task interleavings are asynchronous, that is, their occurrence are initiated by asynchronous events such as hardware interrupts, they do not have an origin in the logic control flow of\ntask execution that blocking system calls do To reproduce these events and interleavings correctly\nin a replay execution, we need to be able to pinpoint and monitor their exact location of occurrence In other words, we need some sort of _unique marker_ to differentiate between different program states The program counter value of the occurrence of the event is a strong candidate for such a unique\nmarker However, as program counter values can be revisited in loops, subroutines and recursive\ncalls, additional information is needed To provide such data, we use the information of the task\ncontext available from the task control block in the TaskSwitchHook This task context is represented\nin the contents of the register set and the task stack To avoid the massive overhead introduced by\nsampling the entire contents of these areas, we use checksums instead [TSHP03] Although these\nchecksums are not truly unique, they strongly aid in differentiating between program states Other solutions to this problem have been proposed, such as hardware instruction counters [CL87]\nand software instruction counters [MCL89], but these are not suitable in the VxWorks case due to lack\nof kernel instrumentation possibilities, large overheads or specialized hardware requirements",
    "Other solutions to this problem have been proposed, such as hardware instruction counters [CL87]\nand software instruction counters [MCL89], but these are not suitable in the VxWorks case due to lack\nof kernel instrumentation possibilities, large overheads or specialized hardware requirements **3 **3 2** **ABB Robotics Instrumentation**\n\n\nIn contrast to the system-level control flow instrumentation, which is handled on the VxWorks RTOS\nlevel, some of the data flow instrumentation needs to be handled on the application level This is due\nto the fact that all of the data used to represent the state of the task execution need to be identified\nexplicitly in the source code and instrumented for recording Such data include static and global\nvariables and structures holding information that can be altered during the entire temporal scope of\nsystem execution As stated in Section 2 3 2, in the Robotics system, these data are grouped together in static structures, individually designed for each task To minimize the amount of information stored in each\ninvocation of a task, we used filters that separated the type of data that was prone of changes during run-time from the type of data that was assigned values during system initialization and kept\nthese values throughout the execution The latter are not recorded during run-time These filters were\nconstructed from information gathered empirically during test-runs of the system To be able to reproduce interaction with an external context and inter-task communication, the\nperipheral I/O and the inter-task communication message queues are instrumented in two operating\nsystem abstraction layers, similar to that described in Section 3",
    "To be able to reproduce interaction with an external context and inter-task communication, the\nperipheral I/O and the inter-task communication message queues are instrumented in two operating\nsystem abstraction layers, similar to that described in Section 3 1 1 1 This solution gives the instrumentation a quality of transparency, making it less sensitive to changes in the application code Fifth Int Workshop on Automated and Algorithmic Debugging\n\n\n**216** **DANIEL SUNDMARK ET AL **\n\n\n\n\n\n\n\n\n\n|Application|Col2|Col3|\n|---|---|---|\n|RTOS abstraction<br>layer|Inter-process<br>abstraction|I/O abstraction<br>layer|\n|**Instrumentation**|**Instrumentation**|**Instrumentation**|\n|**VxWorks**|**VxWorks**|**VxWorks**|\n\n\nFigure 1: Instrumentation layer in the system model However, the part of the data flow recording that is concerned with the reproduction of state preserving structures is performed by probing functions inserted at various locations in the application\ncode A more thorough discussion on how and where these probes should be inserted in the code is\ngiven in our previous papers [TSHP03][HST03] A summarizing overview of system instrumentation\ncan be viewed in Figure 1 In the figure, the gray area represents the instrumentation layer, which is\nslightly integrated into different parts of the RTOS and some application abstraction layers In the figure, the gray area represents the instrumentation layer, which is\nslightly integrated into different parts of the RTOS and some application abstraction layers **3 **3 3** **SAAB Avionics Instrumentation**\n\n\nSince both the Robotics and the Avionics system run on top of VxWorks and the RTOS-level instrumentation is application independent, the instrumentation of the Avionics system-level control flow\nwas implemented in a very similar fashion 3** **SAAB Avionics Instrumentation**\n\n\nSince both the Robotics and the Avionics system run on top of VxWorks and the RTOS-level instrumentation is application independent, the instrumentation of the Avionics system-level control flow\nwas implemented in a very similar fashion However, one aspect had to be taken into account In\ncontrast to the Robotics system, the Avionics system was implemented in Ada As the Ada runtime\nenvironment is added as a layer on top of VxWorks, this layer had to be altered in order to be able to\nmonitor rendezvous and other Ada synchronization mechanisms This instrumentation allowed for\nthe Ada runtime environment to use instrumented versions of the VxWorks synchronization system\ncalls instead of the original versions As for the data flow, this study focused on inter-process communication Messages were logged\nin cyclic buffers, dimensioned on a per-process basis, at the receiver State preserving structures and\nperipheral I/O were not considered as in the ABB Robotics case However, the benchmark figures\nof the inter-process communication recordings give a hint of the overall overhead incorporated in a\nfull-scale instrumentation",
    "However, the benchmark figures\nof the inter-process communication recordings give a hint of the overall overhead incorporated in a\nfull-scale instrumentation **3 4** **Time Machine**\n\n\nOnce the code is properly instrumented, we are able to record any execution of the system in order\nto facilitate replay of that very execution The next step is to implement the mechanisms of the time\nmachine that actually performs the replay These mechanisms were implemented in an add-on to the\nTornado 2 IDE These mechanisms were implemented in an add-on to the\nTornado 2 IDE **3 4 1** **The Historian – Starting the Replay Execution**\n\n\nIn our Time Machine replay system, the task of the Historian is to analyze the data flow- and systemlevel control flow recordings of a reference execution We used basic cyclic FIFO buffers for recording Combined with the fact that the cyclic buffers are of a finite length and memory resources are scarce,\nthis rarely leads to a situation where all recorded information is available at the end of the reference\nexecution As these recordings most often will be of a different temporal length, some sort of pruning\nis needed in order to discard those entries that are out of the consistent temporal scope of all buffers Fifth Int Workshop on Automated and Algorithmic Debugging\n\n\n**REPLAY DEBUGGING OF COMPLEX REAL-TIME SYSTEMS: EXPERIENCES FROM TWO**\n\n**INDUSTRIAL CASE STUDIES** **217**\n\n\nDF buffer C\n\n\nDF buffer B\n\n\nDF buffer A\n\n\nCF buffer\n\n\nt start t min t fail\n\n|Col1|Col2|Col3|Col4|\n|---|---|---|---|\n|||||\n|||||\n|||||\n|||||\n|||||\n|||||\n|||||\n|||||\n\n\n\nT I M E\n\n\nFigure 2: Pruning of buffer entries Entries to the left of t min are discarded",
    "Entries to the left of t min are discarded In other words, all tasks that are to be replayed needs information from both the control flow recording (one per system) and data flow recordings (one per task) Since buffers are dimensioned using a\ndiscrete number of entries and not continuous time, we will practically always end up in a situation\nwhere some buffers cover a longer span of time than others As this information is unusable, it must\nbe detected and discarded This operation is performed by the Historian as depicted in Figure 2 In addition, the Historian has the responsibility to find a consistent state from which the replay\nexecutions can be started [HST03] Again considering Figure 2, if such a starting point exists, it is\nlocated in between t min and t fail, where sufficient information of all instrumented tasks is available When this operation is performed, the Historian sets up the structures in the target system used to\nreproduce the data flow of the reference execution A more elaborate description on how a starting\nstate is found and a replay execution is prepared is presented in a recent paper by Huselius et al [HST03] **3 4 2** **The Time Traveler**\n\n\nAs the replay execution is started, the Time Traveler interacts with the debugger and, given the\ninformation provided by the Historian and the breakpoints visited in the program, allows recreation\nof the system state for any given time in the scope of the replay execution [TSHP03] **3",
    "**3 5** **IDE and Target System Integration**\n\n\nTornado 2 is an integrated development environment including a text editor, a project/workspace\nhandler, a gcc compiler, a target simulator and a gdb-based debugger, capable of performing multithreaded debugging with no requirements on deterministic reproduction of execution orderings 5** **IDE and Target System Integration**\n\n\nTornado 2 is an integrated development environment including a text editor, a project/workspace\nhandler, a gcc compiler, a target simulator and a gdb-based debugger, capable of performing multithreaded debugging with no requirements on deterministic reproduction of execution orderings **3 5 1** **Tornado 2 IDE Architecture and WTX**\n\n\nDebugging in the Tornado 2 environment is performed by means of remote debugging That is,\nthe debugging session is executed on-target and controlled from the development environment via\ncommunication with an on-target task To handle all communication with the target system, a hostbased target server is included in the\nTornado 2 IDE All tools that seek interaction with the target system are able to connect as clients to\nthe target server and issue requests of target operations To provide tool vendors with a possibility\nto create their own add-ons to the Tornado 2 IDE, a programming interface is provided The Wind\nRiver Tool Exchange (WTX) API enables developers to create tools that communicate and interact\ndirectly with the VxWorks target server For the implementation of our Time Machine system, the\nHistorian and the Time Traveler were integrated and implemented as a WTX tool, able to connect\nwith a running target server and to force a replay execution upon the system running on that target Fifth Int Workshop on Automated and Algorithmic Debugging\n\n\n**218** **DANIEL SUNDMARK ET AL **\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Target system, IDE and Time Machine integration The structure of the Tornado 2 IDE, the Time Machine and the target system interactions is depicted\nin Figure 3 **3 5",
    "5 2** **Wdb Task**\n\n\nTo handle the on-target debugging operation, VxWorks provides a dedicated task, the _Wdb task_ This\ntask handles requests of breakpoints, watches, single-stepping and other debugging functions from\nthe IDE These functions are used by the Time Traveler via the WTX interface and the target server\nin order to control the replay execution These functions are used by the Time Traveler via the WTX interface and the target server\nin order to control the replay execution **3 5 3** **Breakpoints**\n\n\nBreakpoints play a central role in the interaction between the time machine and the target system As described by the Deterministic Replay method [TSHP03], breakpoints are set at every point of\npossible task interleaving and as they are encountered in the target system, their occurrence is echoed\nfrom the Wdb task through the WTX and into the event handler of the Time Traveler Based on the\nindividual features of each breakpoint, the state of the replay execution can be deduced and the Time\nTraveler replay engine will force the desired behavior on the target system\n\n\n**3 5 5 4** **Debugging Mode**\n\n\nDebugging in VxWorks can be performed in two different modes: Task mode and system mode The difference is that when in system mode, an encountered breakpoint will halt the entire system,\nincluding the operating system In task mode, a breakpoint will only halt the task that encountered\nit, leaving all other tasks free for execution Ideally, since the investigated applications are pseudoparallel, system mode debugging should\nbe used This would help in guaranteeing the correct ordering of events and task interleavings in\nthe replay execution since no task is able to continue execution and corrupt the system-level control\nflow if the entire system is halted However, we experienced problems when trying to reproduce\nthis ordering in system mode debugging regarding incapability of task suspension This is due to\nthe fact that no tasks can be explicitly suspended from execution by an external operation (such as\nrequests made from the time traveler tool) in system mode debugging In addition, the locking of the\nWdb task substantially complicated communication between the target system and the IDE, making\n\n\nFifth Int Workshop on Automated and Algorithmic Debugging\n\n\n**REPLAY DEBUGGING OF COMPLEX REAL-TIME SYSTEMS: EXPERIENCES FROM TWO**\n\n**INDUSTRIAL CASE STUDIES** **219**\n\n|Instrumentation<br>activity|Bytes per<br>function|CPU<br>Cycles|Time<br>(µs)|CPU utilization<br>(%)|\n|---|---|---|---|---|\n|Data, task 1|360|1992|10|-|\n|Data, task 2|1024|3797|18|0 4|\n|Data, task 3|8192|26545|133|3|\n|Task-switch|-|378|2|0 05|\n|System calls|-|30|0 2|-|\n|Inter-process<br>communication|-|-|5|-|\n\n\n\nFigure 4: Benchmarking resulsts from the Robotics study thorough investigation of the target state more difficult Therefore, task mode debugging is used and\nthe correct ordering of events in the replay execution is explicitly forced upon the system by means\nof the Time Traveler replay engine ### **4 Benchmark**\n\n\nOne of the issues of these case studies was to investigate whether the overhead incorporated by\nsystem-level control flow- and data flow instrumentation was acceptable in a full-scale complex industrial real-time application or not In order to resolve this issue, we performed benchmarks measuring instrumentation mechanism CPU load and memory usage in both systems Since the instrumentation is yet to be optimized and the benchmarking tests are performed under worst-case scenario conditions, many results might be rather pessimistic",
    "Since the instrumentation is yet to be optimized and the benchmarking tests are performed under worst-case scenario conditions, many results might be rather pessimistic **4 1** **ABB Robotics**\n\n\nIn the ABB robotics case, we timestamped the entry and the exit of all instrumentation and recording\nmechanisms This gave us a possibility of extracting the execution time of the software probes In addition, we measured the frequency and recording size of the data flow instrumentation mechanisms The achieved results are presented in Figure 4 The _bytes per function_ column shows the number of\nbytes logged by each iteration of an instrumentation function, and as each task instance has one and\nonly one data flow instrumentation function, this figure also represents the number of bytes stored in\neach task instance of a task The _CPU cycles_ and the _Time_ columns present the execution time spent in\neach instrumentation function and the _CPU utilization_ column shows the percentage of all CPU time\nspent in each instrumentation function Where results are left out, these are discarded due to their\ninsignificant interference with memory or CPU utilization We note that task 3 has a combination of\na high frequency and large state preserving structures, resulting in the largest monitoring overhead\n(3% CPU utilization) **4 2** **SAAB Avionics**\n\n\nAs the data instrumentation in the Avionics system is performed solely on message queues, the data\nflow benchmark is made in a per-queue fashion Since there are major differences in message arrival\nfrequencies and message size between the different queues, only the six most memory-consuming\nmessage queues are presented in the benchmark results Out of the 17 instrumented message queues,\n\n\nFifth Int Workshop on Automated and Algorithmic Debugging\n\n\n**220** **DANIEL SUNDMARK ET AL **\n\n|Instrumentation<br>activity|Msg frequ-<br>ency (Hz)|Data size<br>(bytes)|Bytes/<br>sec |CPU util <br>(%)|\n|---|---|---|---|---|\n|Queue A|60|89|5340|1 9|\n|Queue B|15|9|135|135|\n|Queue C|15|25|375|375|\n|Queue D|15|49|735|735|\n|Queue E|15|281|4215|4215|\n|Queue F|15|21|315|315|\n|System-level<br>control flow|-|-|-|0 03|\n\n\n\nFigure 5: Avionics system benchmarking results these six consume 99% of all message queue memory bandwidth The results of the Avionics benchmarking study are shown in Figure 5, presented in the form of memory utilization for logging (in the\nbytes/second column) and CPU utilization (in the rightmost column) As in the robotics case, it is\nthe combination of a high frequency and a large size of data (such as the one of Queue A and Queue\nE) that has the most significant implications on the instrumentation memory utilization ### **5 Conclusions**\n\n\nWith this paper, we have shown that complex real-time system debugging is feasible using the _De-_\n_terministic Replay_ technique and the _Time Machine_ tool This is true not only for specialized academic\nsystems, but also for full-scale industrial real-time systems Furthermore, we have shown that it is\npossible to achieve a high level of transparency and portability of the method by placing much of\nthe instrumentation in system call-, inter-process communication- and peripheral I/O layers, rather\nthan in the application source code Both case studies presented here indicate a small CPU utilization\noverhead of 0 03–0 05% for system-level control flow instrumentation The data flow instrumentation\nhas proven more temporally substantial, but has stayed in the fully acceptable interval of 1 9–3 0% As for memory utilization, the ABB Robotics instrumentation required a bandwidth of 2 MB/s and\nthe Saab Avionics system called for approximately 12-15 kB/s for both system-level control flow and\ndata flow logging Looking at the size of these systems and the resources available, such a load is\ndefinitely affordable ### **6 Future Work**\n\n\nWe have successfully applied the time machine approach in a number of applications running on\ndifferent operating systems, hardware and debuggers [TSHP03] However, we have learned that it is\nnecessary to carefully analyze the target system’s data flow with respect to what data is re-executed,\nre-transmitted and what data has external origin in order not to forego something that may inhibit\ndeterministic re-execution or that we do not record to much Missing out on information is a serious\nproblem and is something we have begun to address We have started to look into how to automatically derive tight sets of possible data derived from what we actually have recorded Another\nissue that needs to be considered is replay of applications which use vast amounts of information,\ne",
    "Another\nissue that needs to be considered is replay of applications which use vast amounts of information,\ne g , real-time database applications In such applications the “state preserving variables” are very\n\n\nFifth Int Workshop on Automated and Algorithmic Debugging\n\n\n**REPLAY DEBUGGING OF COMPLEX REAL-TIME SYSTEMS: EXPERIENCES FROM TWO**\n\n**INDUSTRIAL CASE STUDIES** **221**\n\n\nsubstantial and require some kind of incremental snapshot algorithm to be manageable ### **References**\n\n\n[AL94] K Audenaert and L Levrouw Interrupt Replay: A Debugging Method for Parallel Programs with Interrupts _Microprocessors and Microsystems_, 18(10):601 – 612, 12 1994 [CAN [+] 01] J -D Choi, B Alpern, T Ngo, M Sridharan, and J Vlissides A Pertrubation-Free Replay\nPlatform for Cross-Optimized Multithreaded Applications In _Proceedings of 15th Parallel_\n_and Distributed Processing Symposium_, page 10, April 2001 [CdKF00] J Chassin de Kergommeaux and A Fagot Execution Replay of Parallel Procedural Programs _Journal of Systems Architecture_, 46(10):835 – 849, 2000 [CL87] T A Cargill and B N Locanthi Locanthi Cheap Hardware Support for Software Debugging and\nProfiling Cheap Hardware Support for Software Debugging and\nProfiling pages 82 – 83, October 1987 [DR92] P Dodd and C V Ravishankar Monitoring and Debugging Distributed Real-Time Programs _Software-Practice and Experience_, 22(10):863 – 877, October 1992 [Gai86] J Gait Gait A Probe Effect in Concurrent Programs _Software – Practice and Experience_,\n16(3):1986, March 1986 [HST03] J Huselius, D Sundmark, and H Thane Starting Conditions for Post-Mortem Debugging using Deterministic Replay of Real-Time Systems In _Proceedings of the 15th Euromicro_\n_Conference on Real-Time Systems (ECRTS03)_, July 2003 [Hus02] J Huselius Debugging Parallel Systems: A State of the Art Report Technical Report 63,\nMälardalen University, Department of Computer Science and Engineering, September\n2002 [LMC87] T J LeBlanc and J M Mellor-Crummey Mellor-Crummey Debugging Parallel Programs with Instant Replay _IEEE Transactions on Computers_, 36(4):471 – 482, April 1987 [MCL89] J Mellor-Crummey and T LeBlanc A Software Instruction Counter In _Proceedings of_\n_the Third International Conference on Architectural Support for Programming Languages and_\n_Operating Systems_, pages 78 – 86 ACM, April 1989 [RDB99] M Ronsse and K De Bosschere RecPlay: A Fully Integrated Practical Record/Replay\nSystem _ACM Transactions on Computer Systems_, 17(2):133 – 152, 5 1999 [Rep02] NIST Report [Rep02] NIST Report The economic impacts of inadequate infrastructure for software testing ,\nMay 2002 [TCO91] K -C Tai, R H Carver, and E E Obaid Debugging Concurrent Ada Programs by Deterministic Execution _IEEE Transactions on Software Engineering_, 17(1):45 – 63, January\n1991 [TFCB90] J P P Tsai, K -Y Fang, H -Y Chen, and Y -D Bi A Noninterference Monitoring and Replay\nMechanism for Real-Time Software Testing and Debugging _IEEE Transactions on Software_\n_Engineering_, 16(8):897 – 916, August 1990 [TH00] H Thane and H Hansson Using Deterministic Replay for Debugging of Distributed\nReal-Time Systems In _Proceedings of the 12th EUROMICRO Conference on Real-Time Sys-_\n_tems_, pages 265 – 272 IEEE Computer Society, June 2000 Fifth Int Workshop on Automated and Algorithmic Debugging\n\n\n**222** **DANIEL SUNDMARK ET AL **\n\n\n[TSHP03] H Thane, D Sundmark, J Huselius, and A Pettersson Replay Debugging of Real-Time\nSystems Using Time Machines In _Parallel and Distributed Systems: Testing and Debugging_\n_(PADTAD)_ ACM, April 2003 Fifth Int Workshop on Automated and Algorithmic Debugging"
  ]
}